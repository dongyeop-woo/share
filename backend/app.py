from __future__ import annotations

import asyncio
import datetime as dt
import os
import time
from typing import Dict, List, Optional, Tuple
from urllib.parse import quote_plus

import contextlib

import logging

import httpx
import requests
import pandas as pd
import numpy as np
import yfinance as yf
import FinanceDataReader as fdr
import feedparser
from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

from services.ai import rank_recommendations, summarize_headline, translate_to_korean
import subprocess
import sys

logger = logging.getLogger(__name__)

# Ollama í´ë¼ì´ì–¸íŠ¸ (ì„ íƒì )
try:
    import ollama
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False
    logger.warning("ollama not available, chatbot will use fallback")

app = FastAPI(
    title="Breaking Share AI API",
    description="AI-powered helpers for the Breaking Share homepage.",
    version="0.1.0",
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


class SummarizeRequest(BaseModel):
    text: str = Field(..., description="ë‰´ìŠ¤ ì „ë¬¸ ë˜ëŠ” ìš”ì•½í•˜ê³  ì‹¶ì€ í•œê¸€ ë¬¸ì¥")
    max_tokens: Optional[int] = Field(
        180,
        ge=32,
        le=512,
        description="ìƒì„± ìš”ì•½ì˜ ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸ 180)",
    )


class SummarizeResponse(BaseModel):
    summary: str


class ChartAnalysisRequest(BaseModel):
    image_url: Optional[str] = Field(None, description="ì°¨íŠ¸ ì´ë¯¸ì§€ URL")
    image_base64: Optional[str] = Field(None, description="ì°¨íŠ¸ ì´ë¯¸ì§€ Base64 ì¸ì½”ë”© ë¬¸ìì—´")
    symbol: Optional[str] = Field(None, description="ì¢…ëª© ì‹¬ë³¼ (ì„ íƒì‚¬í•­)")
    analysis_type: str = Field("full", description="ë¶„ì„ ìœ í˜•: full, pvg, trend, support_resistance")


class ChartAnalysisResponse(BaseModel):
    analysis: str
    pvg_detected: Optional[bool] = None
    trend: Optional[str] = None  # "ìƒìŠ¹", "í•˜ë½", "íš¡ë³´"
    support_levels: Optional[List[float]] = None
    resistance_levels: Optional[List[float]] = None
    recommendations: Optional[List[str]] = None


class ChatRequest(BaseModel):
    message: str = Field(..., description="ì‚¬ìš©ì ë©”ì‹œì§€")
    include_market: bool = Field(True, description="ì‹œì¥ ë°ì´í„° í¬í•¨ ì—¬ë¶€")
    include_news: bool = Field(True, description="ë‰´ìŠ¤ ë°ì´í„° í¬í•¨ ì—¬ë¶€")
    max_news: int = Field(3, ge=0, le=10, description="í¬í•¨í•  ìµœëŒ€ ë‰´ìŠ¤ ê°œìˆ˜")


class ChatSource(BaseModel):
    type: str  # "news", "market", "local_llm"
    title: Optional[str] = None
    url: Optional[str] = None
    content: Optional[str] = None


class ChatResponse(BaseModel):
    reply: str
    sources: List[ChatSource] = Field(default_factory=list)


class RecommendationRequest(BaseModel):
    tickers: List[str] = Field(
        ...,
        min_length=1,
        description="ìŠ¤ì½”ì–´ë§ì„ ì§„í–‰í•  í‹°ì»¤ ëª©ë¡ (ì˜ˆ: ['NVDA', 'AAPL'])",
    )
    weights: Optional[dict[str, float]] = Field(
        None,
        description="ìš”ì†Œë³„ ê°€ì¤‘ì¹˜. eps_growth, revenue_growth, momentum, volatility ì¤‘ ì¼ë¶€/ì „ë¶€ ì§€ì • ê°€ëŠ¥",
    )


class RecommendationItem(BaseModel):
    ticker: str
    composite_score: float
    eps_growth: float
    revenue_growth: float
    momentum: float
    volatility: float


class RecommendationResponse(BaseModel):
    generated_at: dt.datetime
    items: List[RecommendationItem]


class NewsArticle(BaseModel):
    headline: str
    headline_ko: Optional[str] = None
    summary: Optional[str] = None
    summary_ko: Optional[str] = None
    url: str
    source: Optional[str] = None
    published_at: dt.datetime
    symbols: List[str] = Field(default_factory=list)
    image: Optional[str] = None


class MarketQuote(BaseModel):
    symbol: str
    name: str
    current: float
    change: float
    percent: float
    high: Optional[float] = None
    low: Optional[float] = None
    open: Optional[float] = None
    previous_close: Optional[float] = None
    timestamp: dt.datetime


class SymbolSearchResult(BaseModel):
    symbol: str
    description: str
    type: Optional[str] = None
    exchange: Optional[str] = None


class CandleSeries(BaseModel):
    timestamps: List[int]
    opens: List[float]
    highs: List[float]
    lows: List[float]
    closes: List[float]
    volumes: List[float]


class CandleResponse(BaseModel):
    symbol: str
    resolution: str
    data: CandleSeries


# ì°¨íŠ¸ ë¶„ì„ ê´€ë ¨ ëª¨ë¸
class TechnicalIndicator(BaseModel):
    name: str
    value: float
    signal: str  # "buy", "sell", "neutral", "overbought", "oversold"
    description: str


class SupportResistance(BaseModel):
    level: float
    strength: float  # 0-1
    type: str  # "support" or "resistance"


class TrendLine(BaseModel):
    start_price: float
    end_price: float
    start_time: int
    end_time: int
    type: str  # "uptrend", "downtrend", "sideways"


class Pattern(BaseModel):
    name: str
    confidence: float  # 0-1
    description: str
    signal: str  # "bullish", "bearish", "neutral"


class TradingSignal(BaseModel):
    type: str  # "buy", "sell", "hold"
    confidence: float  # 0-1
    entry_price: Optional[float] = None
    target_price: Optional[float] = None
    stop_loss: Optional[float] = None
    reason: str


class ChartAnalysisRequest(BaseModel):
    symbol: str
    resolution: str = "D"
    range_days: int = 60


class ChartAnalysisResponse(BaseModel):
    symbol: str
    technical_indicators: List[TechnicalIndicator]
    support_resistance: List[SupportResistance]
    trend_lines: List[TrendLine]
    patterns: List[Pattern]
    trading_signal: TradingSignal
    risk_analysis: Dict
    summary: str


@app.post("/api/summarize", response_model=SummarizeResponse)
def summarize_news(payload: SummarizeRequest) -> SummarizeResponse:
    try:
        summary = summarize_headline(payload.text, max_tokens=payload.max_tokens)
        return SummarizeResponse(summary=summary)
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=500, detail=str(exc)) from exc


@app.post("/api/analyze-chart", response_model=ChartAnalysisResponse)
async def analyze_chart(payload: ChartAnalysisRequest) -> ChartAnalysisResponse:
    """
    ì°¨íŠ¸ ì´ë¯¸ì§€ë¥¼ AIë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
    PVG(Price Volume Gap), ìƒìŠ¹/í•˜ë½ ë¼ì¸, ì§€ì§€/ì €í•­ì„  ë“±ì„ ë¶„ì„í•©ë‹ˆë‹¤.
    """
    try:
        import base64
        import os
        
        # ì´ë¯¸ì§€ ë°ì´í„° ì¤€ë¹„
        image_data = None
        if payload.image_base64:
            # Base64 ë””ì½”ë”©
            try:
                if payload.image_base64.startswith("data:image"):
                    # data:image/png;base64, í˜•íƒœì¸ ê²½ìš°
                    image_data = payload.image_base64.split(",", 1)[1]
                else:
                    image_data = payload.image_base64
            except Exception as e:
                logger.warning(f"Base64 ë””ì½”ë”© ì‹¤íŒ¨: {e}")
        elif payload.image_url:
            # URLì—ì„œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            try:
                async with httpx.AsyncClient(timeout=10.0) as client:
                    response = await client.get(payload.image_url)
                    if response.status_code == 200:
                        import base64
                        image_data = base64.b64encode(response.content).decode('utf-8')
            except Exception as e:
                logger.warning(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        if not image_data:
            raise HTTPException(status_code=400, detail="ì´ë¯¸ì§€ ë°ì´í„°ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # Ollama Vision APIë¥¼ ì‚¬ìš©í•œ ì°¨íŠ¸ ë¶„ì„
        ollama_model = os.getenv("OLLAMA_MODEL", "qwen2.5:0.5b")
        ollama_host = os.getenv("OLLAMA_HOST", "http://127.0.0.1:11434")
        
        # Vision ëª¨ë¸ì´ ìˆëŠ”ì§€ í™•ì¸ (llava, bakllava ë“±)
        vision_models = ["llava", "bakllava", "qwen2-vl", "llama3.2-vision"]
        model_to_use = ollama_model
        
        # Vision ëª¨ë¸ ì‚¬ìš© ì‹œë„
        for vision_model in vision_models:
            try:
                async with httpx.AsyncClient(timeout=60.0) as client:
                    # ëª¨ë¸ ì¡´ì¬ í™•ì¸
                    check_response = await client.get(f"{ollama_host}/api/tags")
                    if check_response.status_code == 200:
                        available_models = [m.get("name", "") for m in check_response.json().get("models", [])]
                        if any(vm in str(available_models) for vm in vision_models):
                            model_to_use = vision_model
                            break
            except Exception:
                continue
        
        # ì°¨íŠ¸ ë¶„ì„ í”„ë¡¬í”„íŠ¸
        analysis_prompt = f"""ë‹¤ìŒ ì£¼ì‹ ì°¨íŠ¸ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

ë¶„ì„í•´ì•¼ í•  í•­ëª©:
1. PVG (Price Volume Gap): ê°€ê²©ê³¼ ê±°ë˜ëŸ‰ì˜ ê´´ë¦¬ ì—¬ë¶€
2. ì¶”ì„¸ì„ : ìƒìŠ¹ ì¶”ì„¸ì„ , í•˜ë½ ì¶”ì„¸ì„ , íš¡ë³´ ì—¬ë¶€
3. ì§€ì§€ì„ ê³¼ ì €í•­ì„ : ì£¼ìš” ì§€ì§€ì„ ê³¼ ì €í•­ì„ ì˜ ìœ„ì¹˜
4. íŒ¨í„´: ì°¨íŠ¸ íŒ¨í„´ (ì‚¼ê°í˜•, í—¤ë“œì•¤ìˆ„ë”, ë”ë¸”íƒ‘/ë°”í…€ ë“±)
5. ê±°ë˜ ì¶”ì²œ: í˜„ì¬ ì°¨íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ë§¤ìˆ˜/ë§¤ë„/ë³´ìœ  ì¶”ì²œ

ì¢…ëª©: {payload.symbol or "ì•Œ ìˆ˜ ì—†ìŒ"}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
- PVG ê°ì§€: [ì˜ˆ/ì•„ë‹ˆì˜¤ ë° ì„¤ëª…]
- ì¶”ì„¸: [ìƒìŠ¹/í•˜ë½/íš¡ë³´ ë° ì„¤ëª…]
- ì§€ì§€ì„ : [ì£¼ìš” ì§€ì§€ì„  ìœ„ì¹˜ ì„¤ëª…]
- ì €í•­ì„ : [ì£¼ìš” ì €í•­ì„  ìœ„ì¹˜ ì„¤ëª…]
- íŒ¨í„´: [ê°ì§€ëœ ì°¨íŠ¸ íŒ¨í„´ ì„¤ëª…]
- ì¶”ì²œ: [ë§¤ìˆ˜/ë§¤ë„/ë³´ìœ  ë° ì´ìœ ]

ìƒì„¸í•œ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”."""
        
        try:
            async with httpx.AsyncClient(timeout=120.0) as client:
                # Vision ëª¨ë¸ ì‚¬ìš© ì‹œë„
                if model_to_use in vision_models:
                    response = await client.post(
                        f"{ollama_host}/api/generate",
                        json={
                            "model": model_to_use,
                            "prompt": analysis_prompt,
                            "images": [image_data],
                            "stream": False,
                            "options": {
                                "temperature": 0.3,
                                "num_predict": 1000,
                            }
                        }
                    )
                else:
                    # Vision ëª¨ë¸ì´ ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¶„ì„
                    response = await client.post(
                        f"{ollama_host}/api/chat",
                        json={
                            "model": ollama_model,
                            "messages": [
                                {
                                    "role": "system",
                                    "content": "ë‹¹ì‹ ì€ ì£¼ì‹ ì°¨íŠ¸ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì°¨íŠ¸ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ì—¬ PVG, ì¶”ì„¸ì„ , ì§€ì§€/ì €í•­ì„  ë“±ì„ ë¶„ì„í•©ë‹ˆë‹¤."
                                },
                                {
                                    "role": "user",
                                    "content": f"{analysis_prompt}\n\nì°¸ê³ : ì´ë¯¸ì§€ ë°ì´í„°ê°€ ì œê³µë˜ì—ˆì§€ë§Œ í˜„ì¬ ëª¨ë¸ì€ í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ì°¨íŠ¸ ë¶„ì„ ê°€ì´ë“œë¼ì¸ì„ ì œê³µí•´ì£¼ì„¸ìš”."
                                }
                            ],
                            "stream": False,
                            "options": {
                                "temperature": 0.3,
                                "num_predict": 1000,
                            }
                        }
                    )
                
                if response.status_code == 200:
                    data = response.json()
                    if "response" in data:
                        analysis_text = data["response"].strip()
                    elif "message" in data and "content" in data["message"]:
                        analysis_text = data["message"]["content"].strip()
                    else:
                        analysis_text = str(data)
                    
                    # ê°„ë‹¨í•œ íŒŒì‹± (ë” ì •êµí•œ íŒŒì‹±ì€ í•„ìš”ì‹œ ê°œì„ )
                    pvg_detected = "PVG" in analysis_text.upper() or "ê°€ê²©ê³¼ ê±°ë˜ëŸ‰" in analysis_text
                    trend = None
                    if "ìƒìŠ¹" in analysis_text:
                        trend = "ìƒìŠ¹"
                    elif "í•˜ë½" in analysis_text:
                        trend = "í•˜ë½"
                    elif "íš¡ë³´" in analysis_text:
                        trend = "íš¡ë³´"
                    
                    # ì¶”ì²œ ì¶”ì¶œ
                    recommendations = []
                    if "ë§¤ìˆ˜" in analysis_text:
                        recommendations.append("ë§¤ìˆ˜ ê³ ë ¤")
                    if "ë§¤ë„" in analysis_text:
                        recommendations.append("ë§¤ë„ ê³ ë ¤")
                    if "ë³´ìœ " in analysis_text:
                        recommendations.append("ë³´ìœ  ê¶Œì¥")
                    
                    return ChartAnalysisResponse(
                        analysis=analysis_text,
                        pvg_detected=pvg_detected,
                        trend=trend,
                        support_levels=None,  # ì¶”í›„ ê°œì„  ê°€ëŠ¥
                        resistance_levels=None,  # ì¶”í›„ ê°œì„  ê°€ëŠ¥
                        recommendations=recommendations if recommendations else None
                    )
                else:
                    raise HTTPException(status_code=response.status_code, detail=f"Ollama API ì˜¤ë¥˜: {response.text}")
        except httpx.TimeoutException:
            raise HTTPException(status_code=504, detail="ì°¨íŠ¸ ë¶„ì„ ì‹œê°„ ì´ˆê³¼")
        except Exception as e:
            logger.error(f"ì°¨íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise HTTPException(status_code=500, detail=f"ì°¨íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            
    except HTTPException:
        raise
    except Exception as exc:  # noqa: BLE001
        logger.exception("ì°¨íŠ¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
        raise HTTPException(status_code=500, detail=f"ì°¨íŠ¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(exc)}")


@app.post("/api/chat", response_model=ChatResponse)
async def chat_with_llm(payload: ChatRequest) -> ChatResponse:
    """
    LLM API (Ollama)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì±—ë´‡ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    Local LLM (transformers)ì„ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ìš”ì•½ë„ í•¨ê»˜ ì œê³µí•©ë‹ˆë‹¤.
    """
    sources: List[ChatSource] = []
    context_parts = []
    
    # 1. ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘ (ìš”ì²­ëœ ê²½ìš°)
    market_info = ""
    if payload.include_market:
        try:
            async with MARKET_CACHE_LOCK:
                market_quotes = []
                for symbol, name in MARKET_OVERVIEW_SYMBOLS[:3]:  # ì£¼ìš” ì§€ìˆ˜ 3ê°œë§Œ
                    entry = MARKET_CACHE.get(symbol.upper())
                    if entry:
                        quote = entry["quote"]  # type: ignore[index]
                        market_quotes.append(f"{name}: {quote.current:.2f} ({quote.percent:+.2f}%)")
            
            if market_quotes:
                market_info = "ì£¼ìš” ì‹œì¥ ì§€ìˆ˜:\n" + "\n".join(market_quotes)
                sources.append(ChatSource(
                    type="market",
                    title="ì‹œì¥ ë°ì´í„°",
                    content=market_info
                ))
        except Exception as e:
            logger.warning(f"ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    # 2. ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ (ìš”ì²­ëœ ê²½ìš°) - AI ë¶„ì„ ì§ˆë¬¸ì¼ ë•ŒëŠ” ìƒëµí•˜ì—¬ ì†ë„ ê°œì„ 
    news_info = ""
    user_message = payload.message  # ë¨¼ì € ì •ì˜
    is_ai_analysis_question = "ì†ì ˆ" in user_message or "ìµì ˆ" in user_message or "AI ë¶„ì„" in user_message or "ë°˜ë³µ" in user_message
    
    if payload.include_news and not is_ai_analysis_question:
        try:
            articles_list = await _fetch_usa_news()
            articles = articles_list[:payload.max_news] if isinstance(articles_list, list) else list(articles_list)[:payload.max_news]
            if articles:
                news_items = []
                for article in articles:
                    headline = article.headline_ko or article.headline
                    # Local LLMì„ ì‚¬ìš©í•˜ì—¬ ìš”ì•½ (transformers) - íƒ€ì„ì•„ì›ƒ ì„¤ì •
                    summary = article.summary_ko or article.summary
                    if summary:
                        try:
                            # Local LLMìœ¼ë¡œ ìš”ì•½ ìƒì„± (ë¹ ë¥¸ ì‘ë‹µì„ ìœ„í•´ ì§§ê²Œ)
                            summary_short = summarize_headline(summary, max_tokens=30)
                            news_items.append(f"- {headline}: {summary_short}")
                        except Exception:
                            news_items.append(f"- {headline}")
                    else:
                        news_items.append(f"- {headline}")
                    
                    sources.append(ChatSource(
                        type="news",
                        title=headline,
                        url=article.url,
                        content=summary or headline
                    ))
                
                news_info = "ìµœì‹  ê²½ì œ ë‰´ìŠ¤:\n" + "\n".join(news_items)
                sources.append(ChatSource(
                    type="local_llm",
                    title="Local LLM ìš”ì•½",
                    content="transformersë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ìš”ì•½ ìƒì„±"
                ))
        except Exception as e:
            logger.warning(f"ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    # 3. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context = ""
    if market_info:
        context += market_info + "\n\n"
    if news_info:
        context += news_info + "\n\n"
    
    # 4. LLM API (Ollama)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±
    # user_messageëŠ” ì´ë¯¸ ìœ„ì—ì„œ ì •ì˜ë¨
    
    # AI ë¶„ì„ ê¸°ëŠ¥ì— ëŒ€í•œ ì§ˆë¬¸ì¸ì§€ í™•ì¸ (ì´ë¯¸ ìœ„ì—ì„œ í™•ì¸í•¨)
    ai_analysis_context = ""
    if is_ai_analysis_question:
        ai_analysis_context = """
        
TradeNoteì˜ AI ë¶„ì„ ê¸°ëŠ¥ì— ëŒ€í•œ ì •ë³´:
1. "ì†ì ˆ ì‹œ ë°˜ë³µë˜ëŠ” ë¬¸ì œì  ì°¾ê¸°" ê¸°ëŠ¥:
   - ë§¤ë§¤ì¼ì§€ì—ì„œ ì†ì ˆí•œ ê±°ë˜ì˜ 'ì†ì ˆí•œ ì´ìœ 'ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤
   - ìì£¼ ë°˜ë³µë˜ëŠ” íŒ¨í„´ê³¼ ë¬¸ì œì ì„ ì°¾ì•„ë“œë¦½ë‹ˆë‹¤
   - ìµœì†Œ ë§¤ë§¤ì¼ì§€ê°€ 1ê°œ ì´ìƒ ìˆì–´ì•¼ ì‘ë™í•©ë‹ˆë‹¤
   - ì†ì ˆ ì‚¬ìœ ë¥¼ ê¸°ë¡í•˜ë©´ ë” ì •í™•í•œ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤
   
2. "ìµì ˆ ì‹œ ë°˜ë³µë˜ëŠ” ì¢‹ì€ ìŠµê´€ ì°¾ê¸°" ê¸°ëŠ¥:
   - ë§¤ë§¤ì¼ì§€ì—ì„œ ìµì ˆí•œ ê±°ë˜ì˜ 'ìµì ˆí•œ ì´ìœ 'ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤
   - ë°˜ë³µë˜ëŠ” ì¢‹ì€ ìŠµê´€ê³¼ íŒ¨í„´ì„ ì°¾ì•„ë“œë¦½ë‹ˆë‹¤
   - ìµœì†Œ ë§¤ë§¤ì¼ì§€ê°€ 1ê°œ ì´ìƒ ìˆì–´ì•¼ ì‘ë™í•©ë‹ˆë‹¤
   - ìµì ˆ ì‚¬ìœ ë¥¼ ê¸°ë¡í•˜ë©´ ë” ì •í™•í•œ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤

ì´ ê¸°ëŠ¥ë“¤ì€ ì‚¬ìš©ìì˜ ë§¤ë§¤ íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ ê°œì„ ì ì„ ì°¾ì•„ì£¼ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤.
"""
    
    system_prompt = """ë‹¹ì‹ ì€ TradeNoteì˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì‹ ì‹œì¥, ê²½ì œ ë‰´ìŠ¤, ê·¸ë¦¬ê³  TradeNoteì˜ AI ë¶„ì„ ê¸°ëŠ¥ì— ëŒ€í•´ ë„ì›€ì„ ì£¼ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
ì œê³µëœ ì‹œì¥ ë°ì´í„°ì™€ ë‰´ìŠ¤ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
AI ë¶„ì„ ê¸°ëŠ¥ì— ëŒ€í•œ ì§ˆë¬¸ì´ ìˆìœ¼ë©´ ìƒì„¸í•˜ê³  ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”."""
    
    prompt = f"{system_prompt}{ai_analysis_context}\n\n{context}ì‚¬ìš©ì ì§ˆë¬¸: {user_message}\n\në‹µë³€:"
    
    reply = ""
    try:
        if OLLAMA_AVAILABLE:
            # Ollama API ì‚¬ìš© (ë¡œì»¬ LLM API)
            ollama_model = os.getenv("OLLAMA_MODEL", "qwen2.5:0.5b")  # ê¸°ë³¸ê°’: ì‘ì€ ëª¨ë¸
            try:
                # httpxë¥¼ ì‚¬ìš©í•˜ì—¬ Ollama API ì§ì ‘ í˜¸ì¶œ (ë” ì•ˆì •ì )
                ollama_url = os.getenv("OLLAMA_HOST", "http://127.0.0.1:11434")
                # íƒ€ì„ì•„ì›ƒì„ 120ì´ˆë¡œ ëŠ˜ë¦¼ (ëª¨ë¸ ë¡œë”© ì‹œê°„ í¬í•¨)
                async with httpx.AsyncClient(timeout=120.0) as client:
                    response = await client.post(
                        f"{ollama_url}/api/chat",
                        json={
                            "model": ollama_model,
                            "messages": [
                                {"role": "system", "content": system_prompt + ai_analysis_context},
                                {"role": "user", "content": f"{context}ì§ˆë¬¸: {user_message}" if context else f"ì§ˆë¬¸: {user_message}"}
                            ],
                            "options": {
                                "temperature": 0.7,
                                "num_predict": 200 if is_ai_analysis_question else 300,
                            },
                            "stream": False
                        }
                    )
                    if response.status_code == 200:
                        data = response.json()
                        reply = data.get("message", {}).get("content", "")
                        if not reply or len(reply.strip()) == 0:
                            raise ValueError("Ollama ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                        sources.append(ChatSource(
                            type="local_llm",
                            title="Ollama LLM API",
                            content=f"ëª¨ë¸: {ollama_model}"
                        ))
                        logger.info(f"Ollama API ì„±ê³µ: {len(reply)}ì ì‘ë‹µ ìƒì„±")
                    else:
                        raise HTTPException(status_code=response.status_code, detail=f"Ollama API ì˜¤ë¥˜: {response.text}")
            except httpx.TimeoutException:
                logger.warning("Ollama API íƒ€ì„ì•„ì›ƒ, fallback ì‚¬ìš©")
                reply = _generate_fallback_reply(user_message, market_info, news_info)
            except Exception as e:
                logger.warning(f"Ollama API í˜¸ì¶œ ì‹¤íŒ¨: {e}, fallback ì‚¬ìš©")
                reply = _generate_fallback_reply(user_message, market_info, news_info)
        else:
            # Ollamaê°€ ì—†ìœ¼ë©´ ì¦‰ì‹œ fallback ì‚¬ìš©
            reply = _generate_fallback_reply(user_message, market_info, news_info)
    except Exception as e:
        logger.error(f"LLM ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
        reply = _generate_fallback_reply(user_message, market_info, news_info)
    
    if not reply:
        reply = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    return ChatResponse(reply=reply, sources=sources)


def _generate_fallback_reply(message: str, market_info: str, news_info: str) -> str:
    """Ollamaê°€ ì—†ì„ ë•Œ ì‚¬ìš©í•˜ëŠ” ë¹ ë¥¸ fallback ì‘ë‹µ"""
    # ê°„ë‹¨í•œ ì¸ì‚¬ë§ ì²˜ë¦¬
    message_lower = message.lower().strip()
    if message_lower in ["ì•ˆë…•", "ì•ˆë…•í•˜ì„¸ìš”", "hi", "hello", "ì•ˆë…•í•˜ì„¸ìš”!", "ì•ˆë…•!"]:
        return "ì•ˆë…•í•˜ì„¸ìš”! TradeNote AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì‹ ì‹œì¥, ê²½ì œ ë‰´ìŠ¤, ê·¸ë¦¬ê³  AI ë¶„ì„ ê¸°ëŠ¥ì— ëŒ€í•´ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
    
    reply_parts = []
    
    # AI ë¶„ì„ ê¸°ëŠ¥ì— ëŒ€í•œ ì§ˆë¬¸ ì²˜ë¦¬ (ê°€ì¥ ë¹ ë¥´ê²Œ ì‘ë‹µ)
    if "ì†ì ˆ" in message or "ìµì ˆ" in message or "AI ë¶„ì„" in message or "ë°˜ë³µ" in message or "ë¬¸ì œì " in message or "ìŠµê´€" in message:
        if "ì†ì ˆ" in message or "ë¬¸ì œì " in message:
            reply_parts.append("""ì†ì ˆ ì‹œ ë°˜ë³µë˜ëŠ” ë¬¸ì œì  ì°¾ê¸° ê¸°ëŠ¥ì— ëŒ€í•´ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

ğŸ“Š **ê¸°ëŠ¥ ì„¤ëª…:**
ì´ ê¸°ëŠ¥ì€ ë§¤ë§¤ì¼ì§€ì—ì„œ ì†ì ˆí•œ ê±°ë˜ì˜ 'ì†ì ˆí•œ ì´ìœ 'ë¥¼ ë¶„ì„í•˜ì—¬ ìì£¼ ë°˜ë³µë˜ëŠ” íŒ¨í„´ê³¼ ë¬¸ì œì ì„ ì°¾ì•„ë“œë¦½ë‹ˆë‹¤.

ğŸ”§ **ì‚¬ìš© ë°©ë²•:**
1. ë§¤ë§¤ì¼ì§€ì—ì„œ ì†ì ˆ ê±°ë˜ë¥¼ ê¸°ë¡í•˜ì„¸ìš”
2. ê° ê±°ë˜ì˜ 'ì†ì ˆí•œ ì´ìœ 'ë¥¼ ìƒì„¸íˆ ê¸°ë¡í•˜ì„¸ìš”
3. AI ë¶„ì„ í˜ì´ì§€ì—ì„œ 'ì†ì ˆ ì‹œ ë°˜ë³µë˜ëŠ” ë¬¸ì œì  ì°¾ê¸°' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”

ğŸ“ **í•„ìš”í•œ ë°ì´í„°:**
- ìµœì†Œ ë§¤ë§¤ì¼ì§€ 1ê°œ ì´ìƒ
- ì†ì ˆ ê±°ë˜ì˜ ì†ì ˆ ì‚¬ìœ  ê¸°ë¡

ğŸ’¡ **í™œìš© ë°©ë²•:**
ë°˜ë³µë˜ëŠ” ë¬¸ì œì ì„ ë°œê²¬í•˜ë©´, í•´ë‹¹ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ êµ¬ì²´ì ì¸ í–‰ë™ ê³„íšì„ ìˆ˜ë¦½í•˜ì„¸ìš”. ë§¤ë§¤ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ì–´ ì‹¤ìˆ˜ë¥¼ ë°©ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.""")
        elif "ìµì ˆ" in message or "ìŠµê´€" in message:
            reply_parts.append("""ìµì ˆ ì‹œ ë°˜ë³µë˜ëŠ” ì¢‹ì€ ìŠµê´€ ì°¾ê¸° ê¸°ëŠ¥ì— ëŒ€í•´ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

ğŸ“Š **ê¸°ëŠ¥ ì„¤ëª…:**
ì´ ê¸°ëŠ¥ì€ ë§¤ë§¤ì¼ì§€ì—ì„œ ìµì ˆí•œ ê±°ë˜ì˜ 'ìµì ˆí•œ ì´ìœ 'ë¥¼ ë¶„ì„í•˜ì—¬ ë°˜ë³µë˜ëŠ” ì¢‹ì€ ìŠµê´€ê³¼ íŒ¨í„´ì„ ì°¾ì•„ë“œë¦½ë‹ˆë‹¤.

ğŸ”§ **ì‚¬ìš© ë°©ë²•:**
1. ë§¤ë§¤ì¼ì§€ì—ì„œ ìµì ˆ ê±°ë˜ë¥¼ ê¸°ë¡í•˜ì„¸ìš”
2. ê° ê±°ë˜ì˜ 'ìµì ˆí•œ ì´ìœ 'ë¥¼ ìƒì„¸íˆ ê¸°ë¡í•˜ì„¸ìš”
3. AI ë¶„ì„ í˜ì´ì§€ì—ì„œ 'ìµì ˆ ì‹œ ë°˜ë³µë˜ëŠ” ì¢‹ì€ ìŠµê´€ ì°¾ê¸°' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”

ğŸ“ **í•„ìš”í•œ ë°ì´í„°:**
- ìµœì†Œ ë§¤ë§¤ì¼ì§€ 1ê°œ ì´ìƒ
- ìµì ˆ ê±°ë˜ì˜ ìµì ˆ ì‚¬ìœ  ê¸°ë¡

ğŸ’¡ **í™œìš© ë°©ë²•:**
ë°˜ë³µë˜ëŠ” ì¢‹ì€ ìŠµê´€ì„ ë°œê²¬í•˜ë©´, ì´ë¥¼ ë”ìš± ì²´ê³„í™”í•˜ê³  ì¼ê´€ë˜ê²Œ ì ìš©í•˜ì„¸ìš”. ì„±ê³µ íŒ¨í„´ì„ ê°•í™”í•˜ì—¬ ìŠ¹ë¥ ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.""")
        else:
            reply_parts.append("""AI ë¶„ì„ ê¸°ëŠ¥ì— ëŒ€í•´ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

TradeNoteì˜ AI ë¶„ì„ ê¸°ëŠ¥ì€ ë‘ ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤:

1ï¸âƒ£ **ì†ì ˆ ì‹œ ë°˜ë³µë˜ëŠ” ë¬¸ì œì  ì°¾ê¸°**
   - ì†ì ˆ ê±°ë˜ì˜ íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ ê°œì„ ì ì„ ì°¾ìŠµë‹ˆë‹¤
   - ë§¤ë§¤ì¼ì§€ì—ì„œ ì†ì ˆ ì‚¬ìœ ë¥¼ ê¸°ë¡í•˜ë©´ ë” ì •í™•í•œ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤

2ï¸âƒ£ **ìµì ˆ ì‹œ ë°˜ë³µë˜ëŠ” ì¢‹ì€ ìŠµê´€ ì°¾ê¸°**
   - ìµì ˆ ê±°ë˜ì˜ íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ ì„±ê³µ ìš”ì¸ì„ ì°¾ìŠµë‹ˆë‹¤
   - ë§¤ë§¤ì¼ì§€ì—ì„œ ìµì ˆ ì‚¬ìœ ë¥¼ ê¸°ë¡í•˜ë©´ ë” ì •í™•í•œ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤

ğŸ’¡ **íŒ:** ë” ìì„¸í•œ ì •ë³´ë¥¼ ì›í•˜ì‹œë©´ ê° ê¸°ëŠ¥ì˜ ë„ì›€ë§ ë²„íŠ¼(ğŸ’¬)ì„ í´ë¦­í•˜ì„¸ìš”.""")
        return "\n\n".join(reply_parts)
    
    # ì‹œì¥ ë°ì´í„° ê´€ë ¨ ì§ˆë¬¸
    if "ì‹œì¥" in message or "ì§€ìˆ˜" in message or "ì£¼ê°€" in message:
        if market_info:
            reply_parts.append(market_info)
        else:
            reply_parts.append("ì‹œì¥ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # ë‰´ìŠ¤ ê´€ë ¨ ì§ˆë¬¸
    if "ë‰´ìŠ¤" in message or "ì†Œì‹" in message:
        if news_info:
            reply_parts.append(news_info)
        else:
            reply_parts.append("ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # ê¸°ë³¸ ì‘ë‹µ - ë” ìì—°ìŠ¤ëŸ½ê²Œ ê°œì„ 
    if not reply_parts:
        # ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ ì‘ë‹µ
        if any(word in message for word in ["ì‹œì¥", "ì£¼ê°€", "ì§€ìˆ˜", "ì£¼ì‹"]):
            reply_parts.append("ì‹œì¥ ì •ë³´ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì…¨ë„¤ìš”. í˜„ì¬ ì‹œì¥ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ ì •í™•í•œ ë‹µë³€ì„ ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤.")
            reply_parts.append("ì‹œì¥ ë°ì´í„° ê¸°ëŠ¥ì´ í™œì„±í™”ë˜ë©´ ë” ìì„¸í•œ ì •ë³´ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        elif any(word in message for word in ["ë‰´ìŠ¤", "ì†Œì‹", "ì´ìŠˆ"]):
            reply_parts.append("ë‰´ìŠ¤ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì…¨ë„¤ìš”. í˜„ì¬ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ ìµœì‹  ì •ë³´ë¥¼ ì œê³µí•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.")
            reply_parts.append("ë‰´ìŠ¤ ê¸°ëŠ¥ì´ í™œì„±í™”ë˜ë©´ ìµœì‹  ê²½ì œ ë‰´ìŠ¤ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        else:
            reply_parts.append(f"'{message}'ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì…¨ë„¤ìš”.")
            reply_parts.append("ë‹¤ìŒ ì£¼ì œì— ëŒ€í•´ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
            reply_parts.append("â€¢ AI ë¶„ì„ ê¸°ëŠ¥ (ì†ì ˆ/ìµì ˆ íŒ¨í„´ ë¶„ì„)")
            reply_parts.append("â€¢ ì£¼ì‹ ì‹œì¥ ì •ë³´")
            reply_parts.append("â€¢ ê²½ì œ ë‰´ìŠ¤")
            reply_parts.append("ì›í•˜ì‹œëŠ” ì£¼ì œë¥¼ ì„ íƒí•´ì£¼ì‹œë©´ ë” ìì„¸íˆ ì„¤ëª…í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.")
    
    return "\n\n".join(reply_parts)


@app.post("/api/recommendations", response_model=RecommendationResponse)
def generate_recommendations(payload: RecommendationRequest) -> RecommendationResponse:
    try:
        history = yf.download(
            payload.tickers,
            period="6mo",
            interval="1d",
            group_by="ticker",
            auto_adjust=True,
            progress=False,
        )
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=502, detail=f"ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {exc}") from exc

    if history.empty:
        raise HTTPException(status_code=404, detail="ë‹¤ìš´ë¡œë“œí•œ ì‹œì„¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    metrics = []
    for ticker in payload.tickers:
        try:
            ticker_history = history[ticker] if len(payload.tickers) > 1 else history
            close_prices = ticker_history["Close"].dropna()
            if close_prices.empty:
                continue

            returns = close_prices.pct_change().dropna()
            momentum = (close_prices.iloc[-1] / close_prices.iloc[0]) - 1
            volatility = returns.std()

            earnings = yf.Ticker(ticker).get_earnings_dates(limit=4)
            revenue_growth = 0.0
            eps_growth = 0.0
            if earnings is not None and hasattr(earnings, 'empty') and not earnings.empty:
                earnings = earnings.sort_index()
                if "Revenue" in earnings.columns and len(earnings["Revenue"].dropna()) >= 2:
                    revenue_growth = (
                        earnings["Revenue"].iloc[-1] - earnings["Revenue"].iloc[-2]
                    ) / abs(earnings["Revenue"].iloc[-2])
                if "EPS" in earnings.columns and len(earnings["EPS"].dropna()) >= 2:
                    eps_growth = (
                        earnings["EPS"].iloc[-1] - earnings["EPS"].iloc[-2]
                    ) / abs(earnings["EPS"].iloc[-2])

            metrics.append(
                {
                    "ticker": ticker,
                    "momentum": float(momentum),
                    "volatility": float(volatility),
                    "revenue_growth": float(revenue_growth),
                    "eps_growth": float(eps_growth),
                }
            )
        except Exception as exc:  # noqa: BLE001
            raise HTTPException(status_code=500, detail=f"{ticker} ë¶„ì„ ì‹¤íŒ¨: {exc}") from exc

    if not metrics:
        raise HTTPException(status_code=404, detail="í‰ê°€ ê°€ëŠ¥í•œ ì¢…ëª© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    default_weights = {
        "momentum": 0.4,
        "volatility": -0.2,  # ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
        "revenue_growth": 0.2,
        "eps_growth": 0.2,
    }

    if payload.weights:
        default_weights.update(payload.weights)

    ranked = rank_recommendations(metrics, weights=default_weights)
    return RecommendationResponse(
        generated_at=dt.datetime.utcnow(),
        items=[RecommendationItem(**item) for item in ranked],
    )


FINNHUB_NEWS_URL = "https://finnhub.io/api/v1/news"
FINNHUB_QUOTE_URL = "https://finnhub.io/api/v1/quote"
FINNHUB_SEARCH_URL = "https://finnhub.io/api/v1/search"
FINNHUB_CANDLE_URL = "https://finnhub.io/api/v1/stock/candle"

MARKET_OVERVIEW_SYMBOLS = [
    ("SPY", "S&P 500 ETF"),
    ("QQQ", "NASDAQ 100 ETF"),
    ("DIA", "Dow Jones 30 ETF"),
    ("IWM", "Russell 2000 ETF"),
    ("XLF", "Financial Select Sector"),
    ("XLE", "Energy Select Sector"),
    ("XLK", "Technology Select Sector"),
]

CACHE_TTL_SECONDS = 300
QUOTE_CACHE: Dict[str, tuple[MarketQuote, float]] = {}
CANDLE_CACHE: Dict[Tuple[str, str, int], tuple[CandleResponse, float]] = {}
ALPHAVANTAGE_URL = "https://www.alphavantage.co/query"
ALPHA_CACHE_TTL = 300
ALPHA_SERIES_CACHE: Dict[str, tuple[List[dict], float]] = {}
SYMBOL_ALIAS_MAP: Dict[str, Tuple[str, Optional[str]]] = {
    "KOSPI": ("^KS11", "KOSPI ì§€ìˆ˜"),
    "KOSDAQ": ("^KQ11", "KOSDAQ ì§€ìˆ˜"),
    "KOSPI200": ("^KS200", "KOSPI 200"),
    "NASDAQ": ("^IXIC", "NASDAQ Composite"),
    "DOW": ("^DJI", "Dow Jones Industrial Average"),
    "S&P500": ("^GSPC", "S&P 500 Index"),
}
MARKET_REFRESH_INTERVAL = 180
NEWS_REFRESH_INTERVAL = 300
MARKET_CACHE: Dict[str, Dict[str, object]] = {}
MARKET_CACHE_LOCK = asyncio.Lock()
NEWS_CACHE: Dict[str, tuple[List[NewsArticle], float]] = {}
NEWS_CACHE_LOCK = asyncio.Lock()
MARKET_REFRESH_TASK: Optional[asyncio.Task] = None
NEWS_REFRESH_TASK: Optional[asyncio.Task] = None
NEWS_CATEGORIES = ["general"]

# RSS í”¼ë“œ URL ëª©ë¡ (í™•ì¥)
KOREA_NEWS_RSS = [
    "https://www.hankyung.com/feed/economy",  # í•œêµ­ê²½ì œ
    "https://www.mk.co.kr/rss/30000041/",  # ë§¤ì¼ê²½ì œ ê²½ì œ
    "https://biz.chosun.com/rss/site_biz.xml",  # ì¡°ì„ ë¹„ì¦ˆ
    "https://rss.etnews.com/Section901.xml",  # ì „ìì‹ ë¬¸
    "https://www.edaily.co.kr/rss/industry.xml",  # ì´ë°ì¼ë¦¬ ì‚°ì—…
    "https://www.fnnews.com/rss/section?section=economy",  # íŒŒì´ë‚¸ì…œë‰´ìŠ¤ ê²½ì œ
    "https://www.yna.co.kr/rss/economy.xml",  # ì—°í•©ë‰´ìŠ¤ ê²½ì œ
    "https://www.hani.co.kr/rss/economy/",  # í•œê²¨ë ˆ ê²½ì œ
    "https://www.donga.com/rss/economy.xml",  # ë™ì•„ì¼ë³´ ê²½ì œ
    "https://www.joongang.co.kr/rss/economy.xml",  # ì¤‘ì•™ì¼ë³´ ê²½ì œ
    "https://www.seoul.co.kr/rss/economy.xml",  # ì„œìš¸ì‹ ë¬¸ ê²½ì œ
    "https://www.khan.co.kr/rss/economy.xml",  # ê²½í–¥ì‹ ë¬¸ ê²½ì œ
    "https://www.mt.co.kr/rss/",  # ë¨¸ë‹ˆíˆ¬ë°ì´
    "https://www.asiae.co.kr/rss/economy.xml",  # ì•„ì‹œì•„ê²½ì œ
]

USA_NEWS_RSS = [
    "https://rss.cnn.com/rss/money_latest.rss",  # CNN Money
    "https://feeds.bloomberg.com/markets/news.rss",  # Bloomberg Markets
    "https://www.cnbc.com/id/100003114/device/rss/rss.html",  # CNBC News
    "https://www.marketwatch.com/rss/topstories",  # MarketWatch
    "https://feeds.finance.yahoo.com/rss/2.0/headline?s=^GSPC&region=US&lang=en-US",  # Yahoo Finance S&P 500
    "https://feeds.reuters.com/reuters/businessNews",  # Reuters Business
    "https://feeds.reuters.com/reuters/marketsNews",  # Reuters Markets
    "https://www.wsj.com/xml/rss/3_7085.xml",  # Wall Street Journal
    "https://feeds.finance.yahoo.com/rss/2.0/headline?s=^DJI&region=US&lang=en-US",  # Yahoo Finance Dow Jones
    "https://feeds.finance.yahoo.com/rss/2.0/headline?s=^IXIC&region=US&lang=en-US",  # Yahoo Finance NASDAQ
    "https://www.forbes.com/real-time/feed2/",  # Forbes Real-Time
    "https://feeds.fool.com/fool/investing",  # Motley Fool
]


async def _fetch_finnhub_news(category: str) -> List[NewsArticle]:
    api_key = os.getenv("FINNHUB_API_KEY")
    if not api_key:
        raise HTTPException(status_code=500, detail="Finnhub API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    params = {"category": category, "token": api_key}

    async with httpx.AsyncClient(timeout=5.0) as client:
        response = await client.get(FINNHUB_NEWS_URL, params=params)

    if response.status_code == 429:
        raise HTTPException(status_code=429, detail="Finnhub í˜¸ì¶œ ì œí•œì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")

    if response.status_code >= 400:
        raise HTTPException(status_code=502, detail=f"Finnhub ìš”ì²­ ì‹¤íŒ¨: {response.text}")

    data = response.json()
    articles: List[NewsArticle] = []
    for item in data[:12]:
        published_epoch = item.get("datetime")
        published_at = (
            dt.datetime.fromtimestamp(published_epoch, tz=dt.timezone.utc)
            if isinstance(published_epoch, (int, float))
            else dt.datetime.utcnow()
        )
        related_raw = item.get("related") or ""
        symbols = [symbol for symbol in related_raw.split(",") if symbol]
        headline = item.get("headline", "").strip()
        summary = (item.get("summary") or "").strip()

        # Try translation with timeout protection
        headline_ko = None
        summary_ko = None
        try:
            headline_ko = translate_to_korean(headline) or None
        except Exception:
            pass
        try:
            summary_ko = translate_to_korean(summary) or None
        except Exception:
            pass
        
        articles.append(
            NewsArticle(
                headline=headline,
                headline_ko=headline_ko,
                summary=summary or None,
                summary_ko=summary_ko,
                url=item.get("url", ""),
                source=item.get("source"),
                published_at=published_at,
                symbols=symbols,
                image=item.get("image"),
            )
        )

    if not articles:
        raise HTTPException(status_code=404, detail="Finnhubì—ì„œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    return articles


async def _fetch_rss_news(rss_urls: List[str], translate: bool = False) -> List[NewsArticle]:
    """RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    
    Args:
        rss_urls: RSS í”¼ë“œ URL ëª©ë¡
        translate: Trueì´ë©´ ë²ˆì—­ ì‹œë„ (ë¯¸êµ­ ë‰´ìŠ¤ìš©)
    """
    articles: List[NewsArticle] = []
    
    async with httpx.AsyncClient(timeout=10.0, follow_redirects=True) as client:
        for rss_url in rss_urls:
            try:
                response = await client.get(rss_url, headers={"User-Agent": "Mozilla/5.0 (compatible; RSS Reader)"})
                if response.status_code != 200:
                    logger.debug(f"RSS í”¼ë“œ ì‘ë‹µ ì‹¤íŒ¨ ({rss_url}): HTTP {response.status_code}")
                    continue
                
                feed = feedparser.parse(response.text)
                
                # í”¼ë“œê°€ ìœ íš¨í•œì§€ í™•ì¸
                if not hasattr(feed, 'entries') or not feed.entries:
                    logger.debug(f"RSS í”¼ë“œì— í•­ëª©ì´ ì—†ìŒ ({rss_url})")
                    continue
                
                for entry in feed.entries[:30]:  # ê° í”¼ë“œì—ì„œ ìµœëŒ€ 30ê°œ (ë” ë§ì€ ë‰´ìŠ¤ ìˆ˜ì§‘)
                    # ë‚ ì§œ íŒŒì‹±
                    published_at = dt.datetime.utcnow()
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        try:
                            published_at = dt.datetime(*entry.published_parsed[:6], tzinfo=dt.timezone.utc)
                        except Exception:
                            pass
                    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                        try:
                            published_at = dt.datetime(*entry.updated_parsed[:6], tzinfo=dt.timezone.utc)
                        except Exception:
                            pass
                    
                    headline = entry.get("title", "").strip()
                    # summary, description, content ë“±ì—ì„œ ìš”ì•½ ì¶”ì¶œ ì‹œë„
                    summary = (
                        entry.get("summary", "").strip() 
                        or entry.get("description", "").strip()
                        or (entry.get("content", [{}])[0].get("value", "").strip() if entry.get("content") and len(entry.get("content", [])) > 0 else "")
                    )
                    url = entry.get("link", "")
                    source = entry.get("source", {}).get("title", "") if hasattr(entry, "source") else feed.feed.get("title", "")
                    
                    # ì´ë¯¸ì§€ ì¶”ì¶œ
                    image = None
                    if hasattr(entry, 'media_content') and entry.media_content:
                        image = entry.media_content[0].get('url')
                    elif hasattr(entry, 'enclosures') and entry.enclosures:
                        for enc in entry.enclosures:
                            if enc.get('type', '').startswith('image'):
                                image = enc.get('href')
                                break
                    
                    # summaryê°€ ë¹ˆ ë¬¸ìì—´ì´ë©´ Noneìœ¼ë¡œ ì„¤ì •
                    final_summary = summary if summary else None
                    
                    # ë²ˆì—­ì´ í•„ìš”í•œ ê²½ìš° (ë¯¸êµ­ ë‰´ìŠ¤)
                    headline_ko = None
                    summary_ko = None
                    if translate:
                        # ë²ˆì—­ì€ ë‚˜ì¤‘ì— _fetch_usa_newsì—ì„œ ì²˜ë¦¬
                        headline_ko = None
                        summary_ko = None
                    else:
                        # í•œêµ­ ë‰´ìŠ¤ëŠ” ì´ë¯¸ í•œêµ­ì–´
                        headline_ko = headline
                        summary_ko = final_summary
                    
                    articles.append(
                        NewsArticle(
                            headline=headline,
                            headline_ko=headline_ko,
                            summary=final_summary,
                            summary_ko=summary_ko,
                            url=url,
                            source=source,
                            published_at=published_at,
                            symbols=[],
                            image=image,
                        )
                    )
            except Exception as e:
                logger.warning(f"RSS í”¼ë“œ íŒŒì‹± ì‹¤íŒ¨ ({rss_url}): {e}")
                continue
    
    # ë‚ ì§œìˆœìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
    articles.sort(key=lambda x: x.published_at, reverse=True)
    return articles[:50]  # ìµœëŒ€ 50ê°œ ë°˜í™˜ (ë” ë§ì€ ë‰´ìŠ¤ ìˆ˜ì§‘)


async def _fetch_korea_news() -> List[NewsArticle]:
    """í•œêµ­ ê²½ì œ ë‰´ìŠ¤ë¥¼ RSS í”¼ë“œì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    articles = await _fetch_rss_news(KOREA_NEWS_RSS, translate=False)
    
    # ìš”ì•½ì´ ì—†ëŠ” ê¸°ì‚¬ì— ëŒ€í•´ ì²˜ë¦¬
    result = []
    for article in articles:
        # RSS í”¼ë“œì— ìš”ì•½ì´ ì—†ìœ¼ë©´ í—¤ë“œë¼ì¸ì„ ìš”ì•½ìœ¼ë¡œ ì‚¬ìš©
        # (í•œêµ­ ë‰´ìŠ¤ RSS í”¼ë“œëŠ” ëŒ€ë¶€ë¶„ ìš”ì•½ì„ ì œê³µí•˜ì§€ ì•ŠìŒ)
        summary_value = article.summary if article.summary else article.headline
        summary_ko_value = article.summary_ko if article.summary_ko else article.headline
        
        # ìƒˆë¡œìš´ ê°ì²´ ìƒì„± (Pydantic ëª¨ë¸ì€ ë¶ˆë³€ì¼ ìˆ˜ ìˆìŒ)
        result.append(
            NewsArticle(
                headline=article.headline,
                headline_ko=article.headline_ko,
                summary=summary_value,
                summary_ko=summary_ko_value,
                url=article.url,
                source=article.source,
                published_at=article.published_at,
                symbols=article.symbols,
                image=article.image,
            )
        )
    
    return result


async def _fetch_usa_news() -> List[NewsArticle]:
    """ë¯¸êµ­ ê²½ì œ ë‰´ìŠ¤ë¥¼ RSS í”¼ë“œì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    articles = []
    
    # RSS í”¼ë“œ ì‹œë„
    try:
        articles = await _fetch_rss_news(USA_NEWS_RSS, translate=True)
        logger.info(f"RSS í”¼ë“œì—ì„œ {len(articles)}ê°œì˜ ë¯¸êµ­ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.warning(f"RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
    
    # RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í•œ ê²½ìš° Finnhub ì‚¬ìš©
    if not articles or len(articles) == 0:
        logger.info("RSS í”¼ë“œì—ì„œ ë¯¸êµ­ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í•´ Finnhubë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        try:
            finnhub_articles = await _fetch_finnhub_news("general")
            # Finnhub ë‰´ìŠ¤ ì¤‘ ë¯¸êµ­ ê´€ë ¨ ë‰´ìŠ¤ í•„í„°ë§ (ê°„ë‹¨íˆ ì²˜ìŒ 20ê°œ ì‚¬ìš©)
            articles = finnhub_articles[:20]
            logger.info(f"Finnhubì—ì„œ {len(articles)}ê°œì˜ ë¯¸êµ­ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.warning(f"Finnhubì—ì„œë„ ë¯¸êµ­ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {e}")
            # ìµœì†Œí•œ ë¹ˆ ë°°ì—´ì´ ì•„ë‹Œ ê¸°ë³¸ ë©”ì‹œì§€ë¼ë„ ë°˜í™˜
            if not articles:
                return []
    
    # ë¯¸êµ­ ë‰´ìŠ¤ëŠ” ì˜ì–´ì´ë¯€ë¡œ ë²ˆì—­ ì‹œë„ (ë²ˆì—­ ì‹¤íŒ¨í•´ë„ ì›ë¬¸ ë°˜í™˜)
    for article in articles:
        if article.headline and not article.headline_ko:
            try:
                translated = translate_to_korean(article.headline)
                article.headline_ko = translated if translated and translated != article.headline else article.headline
            except Exception as e:
                logger.debug(f"í—¤ë“œë¼ì¸ ë²ˆì—­ ì‹¤íŒ¨ (ì›ë¬¸ ì‚¬ìš©): {e}")
                article.headline_ko = article.headline
        
        if article.summary and not article.summary_ko:
            try:
                translated = translate_to_korean(article.summary)
                article.summary_ko = translated if translated and translated != article.summary else article.summary
            except Exception as e:
                logger.debug(f"ìš”ì•½ ë²ˆì—­ ì‹¤íŒ¨ (ì›ë¬¸ ì‚¬ìš©): {e}")
                article.summary_ko = article.summary
    
    return articles if articles else []


@app.get("/api/news", response_model=List[NewsArticle])
async def get_news(category: str = "general") -> List[NewsArticle]:
    await _ensure_news_cached(category)
    key = category.lower()

    async with NEWS_CACHE_LOCK:
        entry = NEWS_CACHE.get(key)
        if not entry and key != "general":
            entry = NEWS_CACHE.get("general")

    if not entry:
        raise HTTPException(status_code=503, detail="ë‰´ìŠ¤ ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

    return entry[0]


@app.get("/api/news/korea", response_model=List[NewsArticle])
async def get_korea_news() -> List[NewsArticle]:
    """í•œêµ­ ê²½ì œ ë‰´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    key = "korea"
    # ìºì‹œ ë¬´ì‹œí•˜ê³  í•­ìƒ ìµœì‹  ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ìš”ì•½ í¬í•¨)
    articles = await _fetch_korea_news()
    async with NEWS_CACHE_LOCK:
        NEWS_CACHE[key] = (articles, time.time())
    
    return articles


@app.get("/api/news/usa", response_model=List[NewsArticle])
async def get_usa_news() -> List[NewsArticle]:
    """ë¯¸êµ­ ê²½ì œ ë‰´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    key = "usa"
    async with NEWS_CACHE_LOCK:
        entry = NEWS_CACHE.get(key)
        if entry and time.time() - entry[1] < NEWS_REFRESH_INTERVAL:
            return entry[0]
    
    articles = await _fetch_usa_news()
    async with NEWS_CACHE_LOCK:
        NEWS_CACHE[key] = (articles, time.time())
    
    return articles


@app.get("/api/news/symbol/{symbol}", response_model=List[NewsArticle])
async def get_news_by_symbol(symbol: str) -> List[NewsArticle]:
    """íŠ¹ì • ì¢…ëª©ì— ê´€ë ¨ëœ ë‰´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        symbol: ì¢…ëª© ì‹¬ë³¼ (ì˜ˆ: "005930", "AAPL")
    """
    # ì‹¬ë³¼ ì •ê·œí™” (005930.KS -> 005930)
    normalized_symbol = symbol.upper().replace(".KS", "").replace(".KQ", "")
    
    # í•œêµ­ ì¢…ëª©ì¸ì§€ í™•ì¸ (6ìë¦¬ ìˆ«ì)
    is_korean = normalized_symbol.isdigit() and len(normalized_symbol) == 6
    
    # í•œêµ­ ì¢…ëª©ì¸ ê²½ìš° í•œêµ­ ë‰´ìŠ¤, ê·¸ ì™¸ëŠ” ë¯¸êµ­ ë‰´ìŠ¤
    if is_korean:
        all_articles = await _fetch_korea_news()
        # í•œêµ­ ì¢…ëª©ëª… ê°€ì ¸ì˜¤ê¸° (KOREAN_STOCKSëŠ” {name: symbol} í˜•íƒœì´ë¯€ë¡œ ì—­ë°©í–¥ ê²€ìƒ‰)
        stock_name = ""
        for name, sym in KOREAN_STOCKS.items():
            if sym == normalized_symbol:
                stock_name = name
                break
    else:
        all_articles = await _fetch_usa_news()
        stock_name = symbol
    
    # ì¢…ëª©ëª…ì´ë‚˜ ì‹¬ë³¼ì´ í¬í•¨ëœ ë‰´ìŠ¤ í•„í„°ë§
    filtered_articles = []
    logger.info(f"ì¢…ëª©ë³„ ë‰´ìŠ¤ í•„í„°ë§ ì‹œì‘: symbol={normalized_symbol}, stock_name={stock_name}, ì´ ë‰´ìŠ¤ ìˆ˜={len(all_articles)}")
    
    for article in all_articles:
        # symbols í•„ë“œì— ì‹¬ë³¼ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ (ê°€ì¥ í™•ì‹¤í•œ ë°©ë²•)
        if article.symbols and normalized_symbol in article.symbols:
            filtered_articles.append(article)
            logger.debug(f"ì‹¬ë³¼ ë§¤ì¹­: {article.headline[:50]}")
            continue
        
        # í—¤ë“œë¼ì¸ê³¼ ìš”ì•½ í…ìŠ¤íŠ¸ ì¤€ë¹„
        headline_text = (article.headline_ko or article.headline or "").lower()
        summary_text = (article.summary_ko or article.summary or "").lower()
        full_text = f"{headline_text} {summary_text}"
        
        match_targets = []
        if stock_name:
            match_targets.append(stock_name.lower())
        match_targets.extend([normalized_symbol.lower(), symbol.lower()])
        
        if any(target and target in full_text for target in match_targets):
            filtered_articles.append(article)
            logger.debug(f"ì¢…ëª© í‚¤ì›Œë“œ ë§¤ì¹­: {article.headline[:50]}")
            continue
        
    logger.info(f"í•„í„°ë§ ì™„ë£Œ: {len(filtered_articles)}ê°œ ë‰´ìŠ¤ ë°œê²¬")
    
    # í•­ìƒ ì™¸ë¶€ ë‰´ìŠ¤ ì†ŒìŠ¤ì—ì„œë„ ê²€ìƒ‰ (RSS í”¼ë“œë§Œìœ¼ë¡œëŠ” ë¶€ì¡±í•  ìˆ˜ ìˆìŒ)
    try:
        # Google News ê²€ìƒ‰ ë˜ëŠ” NewsAPIë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶”ê°€ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        additional_news = await _fetch_stock_news_from_external(stock_name or symbol, is_korean)
        # ì¤‘ë³µ ì œê±°
        existing_urls = {article.url for article in filtered_articles}
        for news in additional_news:
            if news.url not in existing_urls:
                filtered_articles.append(news)
                existing_urls.add(news.url)
                if len(filtered_articles) >= 20:
                    break
        logger.info(f"ì™¸ë¶€ ë‰´ìŠ¤ ì†ŒìŠ¤ì—ì„œ {len(additional_news)}ê°œ ì¶”ê°€, ì´ {len(filtered_articles)}ê°œ")
    except Exception as e:
        logger.warning(f"ì™¸ë¶€ ë‰´ìŠ¤ ì†ŒìŠ¤ì—ì„œ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    # ìµœëŒ€ 20ê°œê¹Œì§€ ë°˜í™˜ (ë” ë§ì€ ë‰´ìŠ¤ ì œê³µ)
    return filtered_articles[:20]


async def _fetch_stock_news_from_external(stock_query: str, is_korean: bool) -> List[NewsArticle]:
    """ì™¸ë¶€ ë‰´ìŠ¤ ì†ŒìŠ¤ì—ì„œ ì¢…ëª©ë³„ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    
    Args:
        stock_query: ì¢…ëª©ëª… ë˜ëŠ” ì‹¬ë³¼
        is_korean: í•œêµ­ ì¢…ëª© ì—¬ë¶€
    """
    articles: List[NewsArticle] = []
    logger.info(f"ì™¸ë¶€ ë‰´ìŠ¤ ì†ŒìŠ¤ ê²€ìƒ‰ ì‹œì‘: stock_query={stock_query}, is_korean={is_korean}")
    
    try:
        # NewsAPI ì‚¬ìš© (í™˜ê²½ ë³€ìˆ˜ì— NEWS_API_KEYê°€ ìˆëŠ” ê²½ìš°)
        news_api_key = os.getenv("NEWS_API_KEY")
        if news_api_key:
            try:
                # NewsAPIë¡œ ì¢…ëª©ë³„ ë‰´ìŠ¤ ê²€ìƒ‰
                if is_korean:
                    # í•œêµ­ ë‰´ìŠ¤ ê²€ìƒ‰
                    query = f"{stock_query} ì£¼ê°€ OR {stock_query} ì£¼ì‹"
                    url = f"https://newsapi.org/v2/everything"
                    params = {
                        "q": query,
                        "language": "ko",
                        "sortBy": "publishedAt",
                        "pageSize": 20,
                        "apiKey": news_api_key
                    }
                else:
                    # ë¯¸êµ­ ë‰´ìŠ¤ ê²€ìƒ‰
                    query = f"{stock_query} stock OR {stock_query} shares"
                    url = f"https://newsapi.org/v2/everything"
                    params = {
                        "q": query,
                        "language": "en",
                        "sortBy": "publishedAt",
                        "pageSize": 20,
                        "apiKey": news_api_key
                    }
                
                async with httpx.AsyncClient(timeout=10.0) as client:
                    response = await client.get(url, params=params)
                    if response.status_code == 200:
                        data = response.json()
                        if data.get("status") == "ok" and data.get("articles"):
                            for item in data["articles"][:20]:
                                # ì œëª©ê³¼ ì„¤ëª…ì— ì¢…ëª©ëª…ì´ ì •í™•íˆ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                                title = (item.get("title") or "").lower()
                                description = (item.get("description") or "").lower()
                                content = (item.get("content") or "").lower()
                                full_text = f"{title} {description} {content}"
                                
                                stock_lower = stock_query.lower()
                                
                                # ì¢…ëª©ëª…ì´ ì •í™•íˆ í¬í•¨ë˜ì–´ ìˆê³ , ê´€ë ¨ í‚¤ì›Œë“œë„ ìˆëŠ”ì§€ í™•ì¸
                                related_keywords = [
                                    "ì£¼ê°€", "ì£¼ì‹", "ê¸°ì—…", "íšŒì‚¬", "ì¦ê¶Œ", "íˆ¬ì", "ì‹œì¥",
                                    "ìƒìŠ¹", "í•˜ë½", "ê¸‰ë“±", "ê¸‰ë½", "ë§¤ìˆ˜", "ë§¤ë„", "ëª©í‘œê°€",
                                    "ì‹¤ì ", "ì˜ì—…", "ë§¤ì¶œ", "ì´ìµ", "ë°°ë‹¹", "ì¸ìˆ˜", "í•©ë³‘",
                                    "stock", "shares", "price", "trading", "market", "earnings"
                                ]
                                
                                if stock_lower in full_text:
                                    has_related = any(keyword in full_text for keyword in related_keywords)
                                    if has_related or stock_lower in title:
                                        published_at = None
                                        if item.get("publishedAt"):
                                            try:
                                                published_at = dt.datetime.fromisoformat(
                                                    item["publishedAt"].replace("Z", "+00:00")
                                                )
                                            except Exception:
                                                pass
                                        
                                        articles.append(
                                            NewsArticle(
                                                headline=item.get("title", ""),
                                                headline_ko=item.get("title", "") if is_korean else None,
                                                summary=item.get("description", ""),
                                                summary_ko=item.get("description", "") if is_korean else None,
                                                url=item.get("url", ""),
                                                source=item.get("source", {}).get("name", "NewsAPI"),
                                                published_at=published_at,
                                                symbols=None,
                                                image=item.get("urlToImage"),
                                            )
                                        )
            except Exception as e:
                logger.warning(f"NewsAPIì—ì„œ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        # Google News RSS í”¼ë“œ ì‚¬ìš© (API í‚¤ ë¶ˆí•„ìš”) - í•­ìƒ ì‹¤í–‰
        try:
            google_queries = set()
            if is_korean:
                google_queries.update(
                    filter(
                        None,
                        [
                            stock_query,
                            f"{stock_query} ì£¼ê°€",
                            f"{stock_query} ì£¼ì‹",
                            f"{stock_query} ì‹¤ì ",
                            f"{stock_query} ì „ë§",
                            f"{stock_query} ê³µì‹œ",
                            f"{stock_query} ë‰´ìŠ¤",
                        ],
                    )
                )
                lang = "ko"
                region = "KR"
            else:
                google_queries.update(
                    filter(
                        None,
                        [
                            stock_query,
                            f"{stock_query} stock",
                            f"{stock_query} shares",
                            f"{stock_query} earnings",
                            f"{stock_query} forecast",
                            f"{stock_query} news",
                        ],
                    )
                )
                lang = "en"
                region = "US"
            
            async with httpx.AsyncClient(timeout=15.0, follow_redirects=True) as client:
                for query_text in google_queries:
                    query = quote_plus(query_text)
                    google_news_rss = (
                        f"https://news.google.com/rss/search?q={query}&hl={lang}&gl={region}&ceid={region}:{lang}"
                    )
                    logger.info(f"Google News RSS ìš”ì²­: {google_news_rss}")
                    response = await client.get(
                        google_news_rss,
                        headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"},
                    )
                    logger.info(f"Google News RSS ì‘ë‹µ: status={response.status_code}")
                    if response.status_code != 200:
                        continue
        
                    feed = feedparser.parse(response.text)
                    logger.info(f"Google News RSS íŒŒì‹±: entries={len(feed.entries) if hasattr(feed, 'entries') else 0}")
                    if hasattr(feed, 'entries') and feed.entries:
                        logger.info(f"Google News RSSì—ì„œ {len(feed.entries)}ê°œ ë‰´ìŠ¤ ë°œê²¬ (query={query_text})")
                        for entry in feed.entries[:30]:  # ë” ë§ì´ ê°€ì ¸ì™€ì„œ í•„í„°ë§
                            published_at = None
                            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                                try:
                                    published_at = dt.datetime(*entry.published_parsed[:6], tzinfo=dt.timezone.utc)
                                except Exception:
                                    pass
                            
                            url = entry.get("link", "")
                            if not url:
                                continue
                            
                            if not any(a.url == url for a in articles):
                                articles.append(
                                    NewsArticle(
                                        headline=entry.get("title", ""),
                                        headline_ko=entry.get("title", "") if is_korean else None,
                                        summary=entry.get("summary", "") or entry.get("description", ""),
                                        summary_ko=(entry.get("summary", "") or entry.get("description", "")) if is_korean else None,
                                        url=url,
                                        source=entry.get("source", {}).get("title", "Google News") if hasattr(entry, 'source') else "Google News",
                                        published_at=published_at,
                                        symbols=None,
                                        image=None,
                                    )
                                )
                                logger.debug(f"Google News ë§¤ì¹­: {entry.get('title', '')[:50]}")
                                if len(articles) >= 30:
                                    break
        except Exception as e:
            logger.warning(f"Google News RSSì—ì„œ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    except Exception as e:
        logger.error(f"ì™¸ë¶€ ë‰´ìŠ¤ ì†ŒìŠ¤ì—ì„œ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    return articles


def _get_finnhub_api_key() -> str:
    api_key = os.getenv("FINNHUB_API_KEY")
    if not api_key:
        raise HTTPException(status_code=500, detail="Finnhub API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    return api_key


def _extract_error_message(response: httpx.Response) -> str:
    try:
        data = response.json()
        if isinstance(data, dict):
            return data.get("error") or data.get("detail") or response.text
    except Exception:  # noqa: BLE001
        pass
    return response.text


def _get_alpha_api_key() -> str:
    api_key = os.getenv("ALPHAVANTAGE_API_KEY")
    if not api_key:
        raise HTTPException(status_code=500, detail="Alpha Vantage API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    return api_key


async def _fetch_alpha_series(symbol: str) -> List[dict]:
    cache_entry = ALPHA_SERIES_CACHE.get(symbol.upper())
    if cache_entry:
        series, cached_at = cache_entry
        if time.time() - cached_at < ALPHA_CACHE_TTL:
            return series

    params = {
        "function": "TIME_SERIES_DAILY_ADJUSTED",
        "symbol": symbol,
        "apikey": _get_alpha_api_key(),
    }

    async with httpx.AsyncClient(timeout=10.0) as client:
        response = await client.get(ALPHAVANTAGE_URL, params=params)

    if response.status_code >= 400:
        raise HTTPException(status_code=502, detail=f"Alpha Vantage í˜¸ì¶œ ì‹¤íŒ¨: {response.text}")

    payload = response.json()
    series_raw = payload.get("Time Series (Daily)")
    if not series_raw:
        note = payload.get("Note") or payload.get("Information")
        error_message = payload.get("Error Message")
        if note:
            status = 429 if "frequency" in note.lower() else 404
            raise HTTPException(status_code=status, detail=note)
        if error_message:
            raise HTTPException(status_code=404, detail=error_message)
        raise HTTPException(status_code=404, detail=f"{symbol.upper()} ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    series: List[dict] = []
    for date_str, values in series_raw.items():
        try:
            date = dt.datetime.strptime(date_str, "%Y-%m-%d").replace(tzinfo=dt.timezone.utc)
        except ValueError:
            continue

        try:
            record = {
                "date": date,
                "open": float(values["1. open"]),
                "high": float(values["2. high"]),
                "low": float(values["3. low"]),
                "close": float(values["4. close"]),
                "adjusted_close": float(values.get("5. adjusted close", values["4. close"])),
                "volume": float(values["6. volume"]),
            }
        except (KeyError, ValueError):
            continue

        series.append(record)

    if not series:
        raise HTTPException(status_code=404, detail=f"{symbol.upper()} ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    series.sort(key=lambda item: item["date"], reverse=True)
    series = series[:500]
    ALPHA_SERIES_CACHE[symbol.upper()] = (series, time.time())
    return series


def _normalize_symbol(symbol: str, fallback_name: Optional[str] = None) -> Tuple[str, Optional[str]]:
    alias = SYMBOL_ALIAS_MAP.get(symbol.upper())
    if alias:
        provider_symbol, alias_name = alias
        return provider_symbol, alias_name or fallback_name
    return symbol, fallback_name


async def _fetch_quote(symbol: str, name: Optional[str] = None) -> MarketQuote:
    display_symbol = symbol.upper()
    provider_symbol, alias_name = _normalize_symbol(display_symbol, name)
    display_name = alias_name or name

    cache_entry = QUOTE_CACHE.get(provider_symbol.upper())
    if cache_entry:
        cached_quote, cached_at = cache_entry
        if time.time() - cached_at < CACHE_TTL_SECONDS:
            return cached_quote

    try:
        alpha_series = await _fetch_alpha_series(provider_symbol)
        quote = _quote_from_alpha_series(provider_symbol, display_symbol, display_name, alpha_series)
        QUOTE_CACHE[provider_symbol.upper()] = (quote, time.time())
        return quote
    except HTTPException as exc:
        if exc.status_code not in (404, 429):
            raise
        try:
            quote = await asyncio.to_thread(
                _fallback_quote_yfinance, provider_symbol, display_symbol, display_name
            )
            QUOTE_CACHE[provider_symbol.upper()] = (quote, time.time())
            return quote
        except HTTPException:
            raise
        except Exception as fallback_exc:  # noqa: BLE001
            if cache_entry:
                return cache_entry[0]
            raise HTTPException(status_code=exc.status_code, detail=str(fallback_exc)) from fallback_exc


def _quote_from_alpha_series(
    provider_symbol: str, display_symbol: str, display_name: Optional[str], series: List[dict]
) -> MarketQuote:
    latest = series[0]
    prev = series[1] if len(series) > 1 else latest

    current = latest["close"]
    prev_close = prev["close"]

    if prev_close:
        change = current - prev_close
        percent = (change / prev_close) * 100 if prev_close != 0 else 0.0
    else:
        change = 0.0
        percent = 0.0

    return MarketQuote(
        symbol=display_symbol,
        name=display_name or display_symbol,
        current=current,
        change=change,
        percent=percent,
        high=latest["high"],
        low=latest["low"],
        open=latest["open"],
        previous_close=prev_close,
        timestamp=latest["date"],
    )


def _candles_from_series(
    display_symbol: str, series: List[dict], range_days: int, resolution: str = "D"
) -> CandleResponse:
    lookback = max(range_days, 1)
    subset = series[:lookback]
    ordered = list(reversed(subset))

    return CandleResponse(
        symbol=display_symbol,
        resolution=resolution,
        data=CandleSeries(
            timestamps=[int(record["date"].timestamp()) for record in ordered],
            opens=[record["open"] for record in ordered],
            highs=[record["high"] for record in ordered],
            lows=[record["low"] for record in ordered],
            closes=[record["close"] for record in ordered],
            volumes=[record["volume"] for record in ordered],
        ),
    )


async def _refresh_symbol(symbol: str, name: Optional[str] = None) -> Optional[Dict[str, object]]:
    display_symbol = symbol.upper()
    provider_symbol, alias_name = _normalize_symbol(display_symbol, name)
    label = alias_name or name

    try:
        series = await _fetch_alpha_series(provider_symbol)
        quote = _quote_from_alpha_series(provider_symbol, display_symbol, label, series)
    except HTTPException as exc:
        if exc.status_code not in (404, 429):
            logger.warning("Alpha Vantage ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ (%s): %s", display_symbol, exc.detail)
            return None
        logger.info("Alpha Vantage ì œí•œìœ¼ë¡œ yfinance ì‚¬ìš© (%s)", display_symbol)
        try:
            quote = await asyncio.to_thread(
                _fallback_quote_yfinance, provider_symbol, display_symbol, label
            )
            series, candles = await asyncio.to_thread(
                _fallback_candles_yfinance, provider_symbol, display_symbol, "D", 60
            )
        except HTTPException as fallback_exc:
            logger.warning("yfinance ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ (%s): %s", display_symbol, fallback_exc.detail)
            return None
    else:
        candles = _candles_from_series(display_symbol, series, 60)
    CANDLE_CACHE[(provider_symbol.upper(), "D", 60)] = (candles, time.time())
    return {
        "quote": quote,
        "series": series,
        "updated_at": dt.datetime.utcnow(),
    }


async def _ensure_symbol_cached(symbol: str, name: Optional[str] = None) -> None:
    display_symbol = symbol.upper()
    async with MARKET_CACHE_LOCK:
        entry = MARKET_CACHE.get(display_symbol)
        if entry:
            updated_at: dt.datetime = entry["updated_at"]  # type: ignore[assignment]
            if (dt.datetime.utcnow() - updated_at).total_seconds() < CACHE_TTL_SECONDS:
                return

    refreshed = await _refresh_symbol(symbol, name)
    if refreshed:
        async with MARKET_CACHE_LOCK:
            MARKET_CACHE[display_symbol] = refreshed


async def _refresh_market_cache_once() -> None:
    for symbol, name in MARKET_OVERVIEW_SYMBOLS:
        refreshed = await _refresh_symbol(symbol, name)
        if refreshed:
            async with MARKET_CACHE_LOCK:
                MARKET_CACHE[symbol.upper()] = refreshed
        await asyncio.sleep(15)


async def _refresh_news_category(category: str) -> Optional[List[NewsArticle]]:
    try:
        articles = await _fetch_finnhub_news(category)
    except HTTPException as exc:
        logger.warning("ë‰´ìŠ¤ ê°±ì‹  ì‹¤íŒ¨(%s): %s", category, exc.detail)
        return None

    async with NEWS_CACHE_LOCK:
        NEWS_CACHE[category.lower()] = (articles, time.time())
    return articles


async def _refresh_news_cache_once() -> None:
    for category in NEWS_CATEGORIES:
        await _refresh_news_category(category)


async def _market_refresh_loop() -> None:
    while True:
        try:
            await _refresh_market_cache_once()
        except Exception as exc:  # noqa: BLE001
            logger.exception("ì‹œì¥ ë°ì´í„° ê°±ì‹  ë£¨í”„ ì˜¤ë¥˜: %s", exc)
        await asyncio.sleep(MARKET_REFRESH_INTERVAL)


async def _news_refresh_loop() -> None:
    while True:
        try:
            await _refresh_news_cache_once()
        except Exception as exc:  # noqa: BLE001
            logger.exception("ë‰´ìŠ¤ ë°ì´í„° ê°±ì‹  ë£¨í”„ ì˜¤ë¥˜: %s", exc)
        await asyncio.sleep(NEWS_REFRESH_INTERVAL)


async def _ensure_news_cached(category: str) -> None:
    key = category.lower()
    async with NEWS_CACHE_LOCK:
        entry = NEWS_CACHE.get(key)
        if entry and time.time() - entry[1] < NEWS_REFRESH_INTERVAL:
            return
    await _refresh_news_category(category)


@app.get("/api/market/overview", response_model=List[MarketQuote])
async def market_overview() -> List[MarketQuote]:
    async with MARKET_CACHE_LOCK:
        results = []
        missing: List[Tuple[str, str]] = []
        for symbol, name in MARKET_OVERVIEW_SYMBOLS:
            entry = MARKET_CACHE.get(symbol.upper())
            if entry:
                results.append(entry["quote"])  # type: ignore[index]
            else:
                missing.append((symbol, name))

    for symbol, name in missing:
        asyncio.create_task(_refresh_symbol(symbol, name))

    if not results:
        raise HTTPException(status_code=503, detail="ì‹œì¥ ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
    return results


@app.get("/api/market/quote", response_model=MarketQuote)
async def market_quote(symbol: str = Query(..., description="ì¡°íšŒí•  ì¢…ëª© í‹°ì»¤")) -> MarketQuote:
    # í•œêµ­ ì£¼ì‹ì¸ì§€ í™•ì¸
    is_korean_stock = symbol.isdigit() and len(symbol) == 6
    
    if is_korean_stock:
        try:
            return await _fetch_korean_stock_quote(symbol)
        except Exception as e:
            logger.warning(f"í•œêµ­ ì£¼ì‹ ì‹œì„¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}, ë¯¸êµ­ ì£¼ì‹ APIë¡œ í´ë°±")
    
    await _ensure_symbol_cached(symbol)

    async with MARKET_CACHE_LOCK:
        entry = MARKET_CACHE.get(symbol.upper())

    if not entry:
        raise HTTPException(status_code=404, detail=f"{symbol.upper()} ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    return entry["quote"]  # type: ignore[index]


async def _fetch_korean_stock_quote(symbol: str) -> MarketQuote:
    """
    í•œêµ­ ì£¼ì‹ ì‹œì„¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    FinanceDataReaderë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    try:
        # ì¢…ëª© ì½”ë“œ ì •ë¦¬ (005930.KS -> 005930)
        target_symbol = symbol.split('.')[0]
        
        # ìµœê·¼ 7ì¼ ë°ì´í„° ì¡°íšŒ (ì „ì¼ ì¢…ê°€ ê³„ì‚°ì„ ìœ„í•´)
        end_date = dt.datetime.now()
        start_date = end_date - dt.timedelta(days=7)
        
        # FinanceDataReaderëŠ” ë™ê¸° í•¨ìˆ˜ì´ë¯€ë¡œ ì‹¤í–‰
        # ë¹„ë™ê¸° í™˜ê²½ì—ì„œ ë¸”ë¡œí‚¹ì„ í”¼í•˜ê¸° ìœ„í•´ run_in_executor ì‚¬ìš© ê¶Œì¥ë˜ì§€ë§Œ,
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ì§ì ‘ í˜¸ì¶œ (ë¶€í•˜ê°€ í¬ì§€ ì•Šë‹¤ê³  ê°€ì •)
        df = fdr.DataReader(target_symbol, start=start_date, end=end_date)
        
        if df is None or df.empty:
            raise HTTPException(status_code=404, detail=f"{symbol} ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
        last = df.iloc[-1]
        prev = df.iloc[-2] if len(df) > 1 else last
        
        prev_close = float(prev["Close"]) if not pd.isna(prev["Close"]) else None
        current = float(last["Close"])
        
        if prev_close and prev_close != 0:
            change = current - prev_close
            percent = (change / prev_close) * 100
        else:
            change = 0.0
            percent = 0.0
            
        timestamp = df.index[-1]
        if isinstance(timestamp, pd.Timestamp):
            timestamp = timestamp.to_pydatetime()
        if timestamp.tzinfo is None:
            timestamp = timestamp.replace(tzinfo=dt.timezone.utc)
            
        return MarketQuote(
            symbol=symbol,
            name=symbol, # ì´ë¦„ì€ ë³„ë„ ë§¤í•‘ì´ë‚˜ API í•„ìš”, ì¼ë‹¨ ì‹¬ë³¼ë¡œ ëŒ€ì²´
            current=current,
            change=change,
            percent=percent,
            high=float(last["High"]) if not pd.isna(last["High"]) else None,
            low=float(last["Low"]) if not pd.isna(last["Low"]) else None,
            open=float(last["Open"]) if not pd.isna(last["Open"]) else None,
            previous_close=prev_close,
            timestamp=timestamp,
        )
    except Exception as e:
        logger.error(f"í•œêµ­ ì£¼ì‹ ì‹œì„¸ ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"í•œêµ­ ì£¼ì‹ ì‹œì„¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {str(e)}")


# ì£¼ìš” í•œêµ­ ì£¼ì‹ ì¢…ëª©ëª…-ì‹¬ë³¼ ë§¤í•‘
KOREAN_STOCKS = {
    "ì‚¼ì„±ì „ì": "005930",
    "SKí•˜ì´ë‹‰ìŠ¤": "000660",
    "NAVER": "035420",
    "ì¹´ì¹´ì˜¤": "035720",
    "LGì „ì": "066570",
    "í˜„ëŒ€ì°¨": "005380",
    "ê¸°ì•„": "000270",
    "POSCOí™€ë”©ìŠ¤": "005490",
    "ì…€íŠ¸ë¦¬ì˜¨": "068270",
    "KBê¸ˆìœµ": "105560",
    "ì‹ í•œì§€ì£¼": "055550",
    "í•˜ë‚˜ê¸ˆìœµì§€ì£¼": "086790",
    "LGí™”í•™": "051910",
    "ì•„ëª¨ë ˆí¼ì‹œí”½": "090430",
    "ì‚¼ì„±SDI": "006400",
    "í•œí™”ì†”ë£¨ì…˜": "009830",
    "LGìƒí™œê±´ê°•": "051900",
    "ë¡¯ë°ì¼€ë¯¸ì¹¼": "011170",
    "í•œí™”": "000880",
    "ë‘ì‚°ì—ë„ˆë¹Œ": "034020",
}

@app.get("/api/market/search", response_model=List[SymbolSearchResult])
async def market_search(query: str = Query(..., min_length=1, description="ì‹¬ë³¼ ë˜ëŠ” ì¢…ëª©ëª… ê²€ìƒ‰ì–´")) -> List[SymbolSearchResult]:
    results = []
    
    # í•œêµ­ ì£¼ì‹ ê²€ìƒ‰ (6ìë¦¬ ìˆ«ì)
    if query.isdigit() and len(query) == 6:
        results.append(SymbolSearchResult(
            symbol=query,
            description=query,
            type="EQUITY",
            exchange="KRX",
        ))
        return results
    
    # í•œêµ­ ì£¼ì‹ ì¢…ëª©ëª… ê²€ìƒ‰ (í•­ìƒ ì‹¤í–‰)
    query_normalized = query.strip()
    
    # KOREAN_STOCKS ë”•ì…”ë„ˆë¦¬ ì§ì ‘ ê²€ìƒ‰
    for name, symbol in KOREAN_STOCKS.items():
        # ì •í™•í•œ ì¼ì¹˜ ë˜ëŠ” ë¶€ë¶„ ì¼ì¹˜
        try:
            if query_normalized in name or name in query_normalized:
                # ì¤‘ë³µ ì²´í¬
                if not any(r.symbol == symbol for r in results):
                    results.append(SymbolSearchResult(
                        symbol=symbol,
                        description=name,
                        type="EQUITY",
                        exchange="KRX",
                    ))
        except Exception:
            continue
    
    # í•œêµ­ ì£¼ì‹ ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ ë°˜í™˜
    if results:
        return results[:15]
    
    # Finnhub API ì‚¬ìš© (API í‚¤ê°€ ìˆëŠ” ê²½ìš°)
    try:
        api_key = os.getenv("FINNHUB_API_KEY")
        if api_key:
            async with httpx.AsyncClient(timeout=5.0) as client:
                response = await client.get(FINNHUB_SEARCH_URL, params={"q": query, "token": api_key})
            
            if response.status_code == 200:
                payload = response.json()
                finnhub_results = payload.get("result") or []
                
                formatted = [
                    SymbolSearchResult(
                        symbol=item.get("symbol", "").upper(),
                        description=item.get("description", "").strip(),
                        type=item.get("type"),
                        exchange=item.get("exchange"),
                    )
                    for item in finnhub_results
                    if item.get("symbol")
                ]
                results.extend(formatted)
    except Exception as e:
        logger.warning(f"Finnhub ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
    
    # yfinanceë¥¼ ì‚¬ìš©í•œ ê¸°ë³¸ ê²€ìƒ‰ (ë¯¸êµ­ ì£¼ì‹)
    if not results:
        try:
            # ì¼ë°˜ì ì¸ ì£¼ì‹ ì‹¬ë³¼ ê²€ìƒ‰
            ticker = yf.Ticker(query.upper())
            info = ticker.info
            # ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ì¢…ëª©ì¸ì§€ í™•ì¸ (infoê°€ ìˆê³  symbolì´ ìˆëŠ” ê²½ìš°)
            if info and info.get("symbol") and info.get("symbol") != "N/A":
                # ì‹¤ì œ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸ (ìµœì†Œí•œì˜ ì •ë³´ê°€ ìˆì–´ì•¼ í•¨)
                if info.get("longName") or info.get("shortName") or info.get("name"):
                    results.append(SymbolSearchResult(
                        symbol=info.get("symbol", query.upper()),
                        description=info.get("longName") or info.get("shortName") or info.get("name") or query,
                        type="EQUITY",
                        exchange=info.get("exchange", "NASDAQ"),
                    ))
        except Exception as e:
            logger.warning(f"yfinance ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
    
    # ê²€ìƒ‰ ê²°ê³¼ ë°˜í™˜ (ê²€ì¦ì€ ì´ë¯¸ ìˆ˜í–‰ë¨)
    # í•œêµ­ ì£¼ì‹ì€ ì´ë¯¸ ê²€ì¦ë˜ì—ˆê³ , ë¯¸êµ­ ì£¼ì‹ë„ ê²€ì¦ë˜ì—ˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ë°˜í™˜
    logger.info(f"ìµœì¢… ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")
    return results[:15]


# ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° í•¨ìˆ˜ë“¤
def calculate_rsi(prices: pd.Series, period: int = 14) -> pd.Series:
    """RSI (Relative Strength Index) ê³„ì‚°"""
    delta = prices.diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
    rs = gain / loss
    rsi = 100 - (100 / (1 + rs))
    return rsi


def calculate_macd(prices: pd.Series, fast: int = 12, slow: int = 26, signal: int = 9) -> Dict[str, pd.Series]:
    """MACD ê³„ì‚°"""
    ema_fast = prices.ewm(span=fast, adjust=False).mean()
    ema_slow = prices.ewm(span=slow, adjust=False).mean()
    macd_line = ema_fast - ema_slow
    signal_line = macd_line.ewm(span=signal, adjust=False).mean()
    histogram = macd_line - signal_line
    return {
        "macd": macd_line,
        "signal": signal_line,
        "histogram": histogram
    }


def calculate_bollinger_bands(prices: pd.Series, period: int = 20, std_dev: int = 2) -> Dict[str, pd.Series]:
    """ë³¼ë¦°ì € ë°´ë“œ ê³„ì‚°"""
    sma = prices.rolling(window=period).mean()
    std = prices.rolling(window=period).std()
    upper_band = sma + (std * std_dev)
    lower_band = sma - (std * std_dev)
    return {
        "middle": sma,
        "upper": upper_band,
        "lower": lower_band
    }


def calculate_moving_averages(prices: pd.Series) -> Dict[str, pd.Series]:
    """ì´ë™í‰ê· ì„  ê³„ì‚°"""
    return {
        "ma5": prices.rolling(window=5).mean(),
        "ma20": prices.rolling(window=20).mean(),
        "ma60": prices.rolling(window=60).mean(),
        "ma120": prices.rolling(window=120).mean()
    }


def detect_support_resistance(highs: pd.Series, lows: pd.Series, closes: pd.Series, window: int = 20) -> Tuple[List[SupportResistance], List[SupportResistance]]:
    """ì§€ì§€ì„ ê³¼ ì €í•­ì„  íƒì§€ (ê°œì„ ëœ ë²„ì „)"""
    supports = []
    resistances = []
    
    # í”¼ë²— í¬ì¸íŠ¸ ê¸°ë°˜ ì§€ì§€/ì €í•­ì„  íƒì§€
    pivot_window = max(window // 2, 5)  # í”¼ë²— í¬ì¸íŠ¸ ìœˆë„ìš°
    
    # ì§€ì§€ì„  íƒì§€ (ë¡œì»¬ ìµœì €ì )
    for i in range(pivot_window, len(lows) - pivot_window):
        # í”¼ë²— ë¡œìš° (ì–‘ìª½ ëª¨ë‘ë³´ë‹¤ ë‚®ì€ ì )
        if lows.iloc[i] == lows.iloc[i-pivot_window:i+pivot_window+1].min():
            level = float(lows.iloc[i])
            
            # ì£¼ë³€ ê°€ê²©ëŒ€ì—ì„œ ê°™ì€ ë ˆë²¨ ê·¼ì²˜ì˜ í„°ì¹˜ íšŸìˆ˜ ê³„ì‚° (ê°•ë„)
            lookback = min(50, i)  # ìµœëŒ€ 50ì¼ ì „ê¹Œì§€ í™•ì¸
            nearby_touches = 0
            total_candles = 0
            
            for j in range(max(0, i - lookback), min(len(lows), i + 10)):
                total_candles += 1
                # Â±3% ë²”ìœ„ ë‚´ì—ì„œ í„°ì¹˜ í™•ì¸
                if abs(lows.iloc[j] - level) / level <= 0.03:
                    nearby_touches += 1
            
            strength = nearby_touches / total_candles if total_candles > 0 else 0
            
            # ìµœì†Œ ê°•ë„ 0.2 ì´ìƒì¸ ê²ƒë§Œ ìœ ì§€ (ë” ì‹ ë¢°ì„± ìˆëŠ” ì§€ì§€ì„ )
            if strength >= 0.2:
                supports.append(SupportResistance(
                    level=level,
                    strength=min(strength, 1.0),
                    type="support"
                ))
    
    # ì €í•­ì„  íƒì§€ (ë¡œì»¬ ìµœê³ ì )
    for i in range(pivot_window, len(highs) - pivot_window):
        # í”¼ë²— í•˜ì´ (ì–‘ìª½ ëª¨ë‘ë³´ë‹¤ ë†’ì€ ì )
        if highs.iloc[i] == highs.iloc[i-pivot_window:i+pivot_window+1].max():
            level = float(highs.iloc[i])
            
            # ì£¼ë³€ ê°€ê²©ëŒ€ì—ì„œ ê°™ì€ ë ˆë²¨ ê·¼ì²˜ì˜ í„°ì¹˜ íšŸìˆ˜ ê³„ì‚° (ê°•ë„)
            lookback = min(50, i)
            nearby_touches = 0
            total_candles = 0
            
            for j in range(max(0, i - lookback), min(len(highs), i + 10)):
                total_candles += 1
                # Â±3% ë²”ìœ„ ë‚´ì—ì„œ í„°ì¹˜ í™•ì¸
                if abs(highs.iloc[j] - level) / level <= 0.03:
                    nearby_touches += 1
            
            strength = nearby_touches / total_candles if total_candles > 0 else 0
            
            # ìµœì†Œ ê°•ë„ 0.2 ì´ìƒì¸ ê²ƒë§Œ ìœ ì§€
            if strength >= 0.2:
                resistances.append(SupportResistance(
                    level=level,
                    strength=min(strength, 1.0),
                    type="resistance"
                ))
    
    def deduplicate_levels(level_items: List[SupportResistance], reverse: bool = False) -> List[SupportResistance]:
        unique_by_level: Dict[float, SupportResistance] = {}
        for item in level_items:
            # ê°™ì€ ê°€ê²©ëŒ€(Â±2%)ëŠ” í•˜ë‚˜ë¡œ ë¬¶ê³ , ê°•ë„ê°€ ë” ë†’ì€ í•­ëª©ì„ ìœ ì§€
            price_range = item.level * 0.02  # Â±2%
            key = round(item.level / price_range) * price_range
            existing = unique_by_level.get(key)
            if existing is None or item.strength > existing.strength:
                unique_by_level[key] = item
        # ê°•ë„ ìˆœìœ¼ë¡œ ì •ë ¬ í›„ ìƒìœ„ 5ê°œë§Œ ë°˜í™˜
        return sorted(unique_by_level.values(), key=lambda x: x.strength, reverse=True)[:5]
    
    supports = deduplicate_levels(supports, reverse=True)
    resistances = deduplicate_levels(resistances, reverse=False)
    
    return supports, resistances


def detect_trend_lines(timestamps: List[int], prices: pd.Series, window: int = 20) -> List[TrendLine]:
    """ì¶”ì„¸ì„  íƒì§€"""
    trend_lines = []
    
    if len(prices) < window * 2:
        return trend_lines
    
    # ìµœê·¼ ë°ì´í„°ë¡œ ì¶”ì„¸ ë¶„ì„
    recent_prices = prices.iloc[-window:]
    recent_timestamps = timestamps[-window:]
    
    # ì„ í˜• íšŒê·€ë¡œ ì¶”ì„¸ ê³„ì‚°
    x = np.arange(len(recent_prices))
    y = recent_prices.values
    coeffs = np.polyfit(x, y, 1)
    slope = coeffs[0]
    
    start_price = float(recent_prices.iloc[0])
    end_price = float(recent_prices.iloc[-1])
    
    if slope > 0:
        trend_type = "uptrend"
    elif slope < 0:
        trend_type = "downtrend"
    else:
        trend_type = "sideways"
    
    trend_lines.append(TrendLine(
        start_price=start_price,
        end_price=end_price,
        start_time=recent_timestamps[0],
        end_time=recent_timestamps[-1],
        type=trend_type
    ))
    
    return trend_lines


def detect_patterns(highs: pd.Series, lows: pd.Series, closes: pd.Series) -> List[Pattern]:
    """ì°¨íŠ¸ íŒ¨í„´ íƒì§€"""
    patterns = []
    
    if len(closes) < 20:
        return patterns
    
    recent_closes = closes.iloc[-20:]
    recent_highs = highs.iloc[-20:]
    recent_lows = lows.iloc[-20:]
    
    # í—¤ë“œì•¤ìˆ„ë” íŒ¨í„´ (ê°„ë‹¨í•œ ë²„ì „)
    if len(recent_highs) >= 5:
        peaks = []
        for i in range(2, len(recent_highs) - 2):
            if recent_highs.iloc[i] > recent_highs.iloc[i-1] and recent_highs.iloc[i] > recent_highs.iloc[i+1]:
                peaks.append((i, recent_highs.iloc[i]))
        
        if len(peaks) >= 3:
            # í—¤ë“œì•¤ìˆ„ë” íŒ¨í„´ í™•ì¸
            peaks_sorted = sorted(peaks, key=lambda x: x[1], reverse=True)
            if peaks_sorted[0][1] > peaks_sorted[1][1] and peaks_sorted[0][1] > peaks_sorted[2][1]:
                patterns.append(Pattern(
                    name="í—¤ë“œì•¤ìˆ„ë”",
                    confidence=0.6,
                    description="í•˜ë½ ë°˜ì „ íŒ¨í„´ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.",
                    signal="bearish"
                ))
    
    # ì‚¼ê°í˜• íŒ¨í„´
    if len(recent_highs) >= 10:
        high_trend = np.polyfit(range(len(recent_highs)), recent_highs.values, 1)[0]
        low_trend = np.polyfit(range(len(recent_lows)), recent_lows.values, 1)[0]
        
        if high_trend < 0 and low_trend > 0:
            patterns.append(Pattern(
                name="ìˆ˜ë ´ ì‚¼ê°í˜•",
                confidence=0.5,
                description="ê°€ê²©ì´ ìˆ˜ë ´í•˜ê³  ìˆìœ¼ë©° ê³§ ë°©í–¥ì„±ì´ ê²°ì •ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                signal="neutral"
            ))
    
    return patterns


def generate_trading_signal(
    rsi: float,
    macd: Dict,
    closes: pd.Series,
    supports: List[SupportResistance],
    resistances: List[SupportResistance],
    patterns: List[Pattern]
) -> TradingSignal:
    """ë§¤ë§¤ ì‹ í˜¸ ìƒì„±"""
    current_price = float(closes.iloc[-1])
    signals = []
    confidence_sum = 0
    
    # RSI ì‹ í˜¸
    if rsi < 30:
        signals.append(("buy", 0.3, "RSIê°€ ê³¼ë§¤ë„ êµ¬ê°„ì…ë‹ˆë‹¤."))
    elif rsi > 70:
        signals.append(("sell", 0.3, "RSIê°€ ê³¼ë§¤ìˆ˜ êµ¬ê°„ì…ë‹ˆë‹¤."))
    
    # MACD ì‹ í˜¸
    if macd["macd"].iloc[-1] > macd["signal"].iloc[-1] and macd["histogram"].iloc[-1] > 0:
        signals.append(("buy", 0.25, "MACDê°€ ìƒìŠ¹ ì‹ í˜¸ë¥¼ ë³´ì…ë‹ˆë‹¤."))
    elif macd["macd"].iloc[-1] < macd["signal"].iloc[-1] and macd["histogram"].iloc[-1] < 0:
        signals.append(("sell", 0.25, "MACDê°€ í•˜ë½ ì‹ í˜¸ë¥¼ ë³´ì…ë‹ˆë‹¤."))
    
    # ì§€ì§€/ì €í•­ì„  ì‹ í˜¸
    if supports:
        nearest_support = max([s.level for s in supports if s.level < current_price], default=None)
        if nearest_support and current_price <= nearest_support * 1.02:
            signals.append(("buy", 0.2, f"ì§€ì§€ì„  ê·¼ì²˜ì—ì„œ ë§¤ìˆ˜ ê¸°íšŒì…ë‹ˆë‹¤."))
    
    if resistances:
        nearest_resistance = min([r.level for r in resistances if r.level > current_price], default=None)
        if nearest_resistance and current_price >= nearest_resistance * 0.98:
            signals.append(("sell", 0.2, f"ì €í•­ì„  ê·¼ì²˜ì—ì„œ ë§¤ë„ ê¸°íšŒì…ë‹ˆë‹¤."))
    
    # íŒ¨í„´ ì‹ í˜¸
    for pattern in patterns:
        if pattern.signal == "bullish":
            signals.append(("buy", pattern.confidence * 0.15, pattern.description))
        elif pattern.signal == "bearish":
            signals.append(("sell", pattern.confidence * 0.15, pattern.description))
    
    # ì‹ í˜¸ ì§‘ê³„
    buy_score = sum([conf for sig, conf, _ in signals if sig == "buy"])
    sell_score = sum([conf for sig, conf, _ in signals if sig == "sell"])
    
    if buy_score > sell_score and buy_score > 0.3:
        signal_type = "buy"
        confidence = min(buy_score, 1.0)
        target_price = current_price * 1.1 if resistances else current_price * 1.05
        stop_loss = current_price * 0.95
    elif sell_score > buy_score and sell_score > 0.3:
        signal_type = "sell"
        confidence = min(sell_score, 1.0)
        target_price = current_price * 0.9 if supports else current_price * 0.95
        stop_loss = current_price * 1.05
    else:
        signal_type = "hold"
        confidence = 0.5
        target_price = None
        stop_loss = None
    
    reason = " | ".join([desc for sig, _, desc in signals if sig == signal_type]) or "í˜„ì¬ ì¶”ì„¸ ìœ ì§€"
    
    return TradingSignal(
        type=signal_type,
        confidence=confidence,
        entry_price=current_price if signal_type != "hold" else None,
        target_price=target_price,
        stop_loss=stop_loss,
        reason=reason
    )


@app.post("/api/chart/analyze", response_model=ChartAnalysisResponse)
async def analyze_chart_data(payload: ChartAnalysisRequest) -> ChartAnalysisResponse:
    """ì°¨íŠ¸ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ê¸°ìˆ ì  ì§€í‘œ, íŒ¨í„´, ì‹ í˜¸ ë“±ì„ ì œê³µ"""
    try:
        # ìº”ë“¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        is_korean_stock = payload.symbol.isdigit() and len(payload.symbol) == 6
        
        if is_korean_stock:
            candle_response = await _fetch_korean_stock_candles(payload.symbol, payload.resolution, payload.range_days)
        else:
            # ë¯¸êµ­ ì£¼ì‹ì€ market_candles ì—”ë“œí¬ì¸íŠ¸ì™€ ë™ì¼í•œ ë¡œì§ ì‚¬ìš©
            candle_response = await market_candles(
                symbol=payload.symbol,
                resolution=payload.resolution,
                range_days=payload.range_days
            )
        
        if not candle_response.data.timestamps:
            raise HTTPException(status_code=404, detail="ì°¨íŠ¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # DataFrame ìƒì„±
        df = pd.DataFrame({
            "timestamp": candle_response.data.timestamps,
            "open": candle_response.data.opens,
            "high": candle_response.data.highs,
            "low": candle_response.data.lows,
            "close": candle_response.data.closes,
            "volume": candle_response.data.volumes
        })
        
        df["date"] = pd.to_datetime(df["timestamp"], unit="s")
        df = df.set_index("date")
        
        closes = df["close"]
        highs = df["high"]
        lows = df["low"]
        volumes = df["volume"]
        
        # ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
        rsi = calculate_rsi(closes)
        rsi_current = float(rsi.iloc[-1]) if not pd.isna(rsi.iloc[-1]) else 50.0
        
        macd = calculate_macd(closes)
        macd_current = float(macd["macd"].iloc[-1]) if not pd.isna(macd["macd"].iloc[-1]) else 0.0
        
        bb = calculate_bollinger_bands(closes)
        bb_current = {
            "upper": float(bb["upper"].iloc[-1]) if not pd.isna(bb["upper"].iloc[-1]) else closes.iloc[-1] * 1.1,
            "middle": float(bb["middle"].iloc[-1]) if not pd.isna(bb["middle"].iloc[-1]) else closes.iloc[-1],
            "lower": float(bb["lower"].iloc[-1]) if not pd.isna(bb["lower"].iloc[-1]) else closes.iloc[-1] * 0.9
        }
        
        mas = calculate_moving_averages(closes)
        
        # ê¸°ìˆ ì  ì§€í‘œ ì‹ í˜¸ íŒë‹¨
        rsi_signal = "oversold" if rsi_current < 30 else "overbought" if rsi_current > 70 else "neutral"
        rsi_desc = f"RSI: {rsi_current:.2f} - {'ê³¼ë§¤ë„' if rsi_signal == 'oversold' else 'ê³¼ë§¤ìˆ˜' if rsi_signal == 'overbought' else 'ì¤‘ë¦½'}"
        
        macd_signal = "buy" if macd["macd"].iloc[-1] > macd["signal"].iloc[-1] else "sell" if macd["macd"].iloc[-1] < macd["signal"].iloc[-1] else "neutral"
        macd_desc = f"MACD: {macd_current:.2f} - {'ìƒìŠ¹ ì‹ í˜¸' if macd_signal == 'buy' else 'í•˜ë½ ì‹ í˜¸' if macd_signal == 'sell' else 'ì¤‘ë¦½'}"
        
        bb_signal = "overbought" if closes.iloc[-1] > bb_current["upper"] else "oversold" if closes.iloc[-1] < bb_current["lower"] else "neutral"
        bb_desc = f"ë³¼ë¦°ì € ë°´ë“œ: í˜„ì¬ê°€ê°€ {'ìƒë‹¨' if bb_signal == 'overbought' else 'í•˜ë‹¨' if bb_signal == 'oversold' else 'ì¤‘ê°„'} ë°´ë“œì— ìœ„ì¹˜"
        
        technical_indicators = [
            TechnicalIndicator(name="RSI", value=rsi_current, signal=rsi_signal, description=rsi_desc),
            TechnicalIndicator(name="MACD", value=macd_current, signal=macd_signal, description=macd_desc),
            TechnicalIndicator(name="Bollinger Bands", value=closes.iloc[-1], signal=bb_signal, description=bb_desc),
        ]
        
        # ì§€ì§€/ì €í•­ì„  íƒì§€
        supports, resistances = detect_support_resistance(highs, lows, closes)
        all_sr = supports + resistances
        
        # ì¶”ì„¸ì„  íƒì§€
        trend_lines = detect_trend_lines(candle_response.data.timestamps, closes)
        
        # íŒ¨í„´ íƒì§€
        patterns = detect_patterns(highs, lows, closes)
        
        # ë§¤ë§¤ ì‹ í˜¸ ìƒì„±
        trading_signal = generate_trading_signal(rsi_current, macd, closes, supports, resistances, patterns)
        
        # ë¦¬ìŠ¤í¬ ë¶„ì„
        volatility = float(closes.pct_change().std() * np.sqrt(252))  # ì—°ê°„ ë³€ë™ì„±
        risk_level = "high" if volatility > 0.3 else "medium" if volatility > 0.2 else "low"
        
        risk_analysis = {
            "volatility": round(volatility * 100, 2),
            "risk_level": risk_level,
            "current_price": float(closes.iloc[-1]),
            "price_range_52w": {
                "high": float(highs.max()),
                "low": float(lows.min())
            }
        }
        
        # ìš”ì•½ ìƒì„±
        summary_parts = []
        summary_parts.append(f"í˜„ì¬ê°€: {closes.iloc[-1]:.2f}")
        summary_parts.append(f"RSI: {rsi_current:.1f} ({rsi_signal})")
        summary_parts.append(f"ì¶”ì„¸: {trend_lines[0].type if trend_lines else 'ë¶ˆëª…í™•'}")
        summary_parts.append(f"ë§¤ë§¤ ì‹ í˜¸: {trading_signal.type.upper()} (ì‹ ë¢°ë„: {trading_signal.confidence*100:.0f}%)")
        if patterns:
            summary_parts.append(f"íŒ¨í„´: {', '.join([p.name for p in patterns])}")
        
        summary = " | ".join(summary_parts)
        
        return ChartAnalysisResponse(
            symbol=payload.symbol,
            technical_indicators=technical_indicators,
            support_resistance=all_sr,
            trend_lines=trend_lines,
            patterns=patterns,
            trading_signal=trading_signal,
            risk_analysis=risk_analysis,
            summary=summary
        )
        
    except Exception as e:
        logger.error(f"ì°¨íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ì°¨íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")


@app.get("/api/technical-indicators/test-additional")
async def test_additional_indicators(symbol: str = Query(..., description="ì¢…ëª© ì‹¬ë³¼")):
    """
    ì¶”ê°€ ê¸°ìˆ ì  ì§€í‘œ ì‹ ë¢°ë„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (RSI, MACD, ë³¼ë¦°ì € ë°´ë“œ, ë¦¬ìŠ¤í¬)
    """
    try:
        script_path = os.path.join(os.path.dirname(__file__), "tests", "test_additional_indicators.py")
        # ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹œ backend ë””ë ‰í† ë¦¬ë¥¼ ì‘ì—… ë””ë ‰í† ë¦¬ë¡œ ì„¤ì •
        backend_dir = os.path.dirname(__file__)
        env = os.environ.copy()
        env['PYTHONIOENCODING'] = 'utf-8'
        result = subprocess.run(
            [sys.executable, script_path, "--symbol", symbol, "--format", "text"],
            capture_output=True,
            text=True,
            encoding='utf-8',
            errors='replace',
            timeout=180,
            env=env,
            cwd=backend_dir  # ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ backendë¡œ ì„¤ì •
        )
        
        if result.returncode != 0:
            return {
                "success": False,
                "error": result.stderr or "í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨",
                "report": result.stdout
            }
        
        return {
            "success": True,
            "format": "text",
            "report": result.stdout
        }
    except subprocess.TimeoutExpired:
        return {
            "success": False,
            "error": "í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œê°„ ì´ˆê³¼ (3ë¶„ ì´ìƒ ì†Œìš”)"
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }

@app.get("/api/technical-indicators/test")
async def test_technical_indicators(symbol: str = Query(..., description="ì¢…ëª© ì‹¬ë³¼")):
    """
    ê¸°ìˆ ì  ì§€í‘œ ì‹ ë¢°ë„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    """
    try:
        # test_technical_indicators.py ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
        script_path = os.path.join(os.path.dirname(__file__), "tests", "test_technical_indicators.py")
        
        # Python ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        # Windowsì—ì„œ UTF-8 ì¸ì½”ë”© ê°•ì œ
        env = os.environ.copy()
        env['PYTHONIOENCODING'] = 'utf-8'
        
        # ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹œ backend ë””ë ‰í† ë¦¬ë¥¼ ì‘ì—… ë””ë ‰í† ë¦¬ë¡œ ì„¤ì •
        backend_dir = os.path.dirname(__file__)
        result = subprocess.run(
            [sys.executable, script_path, "--symbol", symbol, "--format", "text"],
            capture_output=True,
            text=True,
            encoding='utf-8',
            errors='replace',  # ì¸ì½”ë”© ì˜¤ë¥˜ ì‹œ ëŒ€ì²´ ë¬¸ì ì‚¬ìš©
            timeout=180,  # 3ë¶„ íƒ€ì„ì•„ì›ƒ
            env=env,
            cwd=backend_dir  # ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ backendë¡œ ì„¤ì •
        )
        
        if result.returncode != 0:
            return {
                "success": False,
                "error": result.stderr or "í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨",
                "output": result.stdout
            }
        
        # í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ë°˜í™˜
        return {
            "success": True,
            "format": "text",
            "report": result.stdout
        }
            
    except subprocess.TimeoutExpired:
        return {
            "success": False,
            "error": "í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. (ìµœëŒ€ 3ë¶„)"
        }
    except Exception as e:
        logger.error(f"ê¸°ìˆ ì  ì§€í‘œ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}", exc_info=True)
        return {
            "success": False,
            "error": str(e)
        }


@app.get("/api/market/orderbook")
async def get_orderbook(symbol: str):
    """
    ë„¤ì´ë²„ ê¸ˆìœµì—ì„œ í˜¸ê°€ ë°ì´í„° ìŠ¤í¬ë˜í•‘ (ì‹¤ì‹œê°„ ê·¼ì ‘)
    """
    try:
        # Run in thread pool to avoid blocking
        loop = asyncio.get_event_loop()
        data = await loop.run_in_executor(None, _fetch_naver_orderbook, symbol)
        return data
    except Exception as e:
        logger.error(f"Failed to fetch orderbook for {symbol}: {e}")
        raise HTTPException(status_code=500, detail="í˜¸ê°€ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

def _fetch_naver_orderbook(symbol: str) -> Dict:
    url = f"https://finance.naver.com/item/sise.naver?code={symbol}"
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    try:
        # pandas read_html uses urllib/requests internally
        # We use requests to get text first to ensure headers are passed if needed, 
        # but pd.read_html can take a URL directly too. 
        # Using requests explicitly is safer for headers.
        response = requests.get(url, headers=headers)
        dfs = pd.read_html(response.text)
        
        if len(dfs) < 4:
            raise ValueError("Orderbook table not found")
            
        df = dfs[3] # Table 3 is usually the orderbook
        
        asks = []
        bids = []
        
        # Asks: Rows 1-5 (indices 1-5)
        # Column 1: Price, Column 0: Volume
        for i in range(1, 6):
            try:
                price = df.iloc[i, 1]
                volume = df.iloc[i, 0]
                if pd.notna(price) and pd.notna(volume):
                    asks.append({
                        "price": int(price),
                        "volume": int(volume),
                        "type": "ask"
                    })
            except:
                pass
                
        # Bids: Rows 8-12 (indices 8-12)
        # Column 3: Price, Column 4: Volume
        for i in range(8, 13):
            try:
                price = df.iloc[i, 3]
                volume = df.iloc[i, 4]
                if pd.notna(price) and pd.notna(volume):
                    bids.append({
                        "price": int(price),
                        "volume": int(volume),
                        "type": "bid"
                    })
            except:
                pass
        
        # Sort asks descending (highest price first - standard for display stack)
        # But for the list, we usually want lowest ask first? 
        # The table has 95300 -> 94900 (descending).
        # Usually orderbook displays:
        # Asks (High -> Low)
        # Bids (High -> Low)
        # So the table order is already correct for visual stacking.
        
        return {
            "symbol": symbol,
            "asks": asks, # 95300, 95200, ...
            "bids": bids  # 94800, 94700, ...
        }
        
    except Exception as e:
        logger.error(f"Error parsing orderbook: {e}")
        raise e

@app.get("/api/market/candles", response_model=CandleResponse)
async def market_candles(
    symbol: str = Query(..., description="ì¡°íšŒí•  ì¢…ëª© í‹°ì»¤"),
    resolution: str = Query("15", description="Finnhub ìº”ë“¤ í•´ìƒë„ (1,5,15,30,60,240,D,W,M)"),
    range_days: int = Query(5, ge=1, le=5000, description="ì¡°íšŒ ê¸°ê°„(ì¼)"),
) -> CandleResponse:
    # í•œêµ­ ì£¼ì‹ì¸ì§€ í™•ì¸ (6ìë¦¬ ìˆ«ìë¡œ ì‹œì‘)
    is_korean_stock = symbol.isdigit() and len(symbol) == 6
    
    if is_korean_stock:
        # í•œêµ­ ì£¼ì‹ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        try:
            return await _fetch_korean_stock_candles(symbol, resolution, range_days)
        except HTTPException:
            # HTTPExceptionì€ ê·¸ëŒ€ë¡œ ì „ë‹¬
            raise
        except Exception as e:
            logger.error(f"í•œêµ­ ì£¼ì‹ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            # í•œêµ­ ì£¼ì‹ ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ì¼ë´‰ìœ¼ë¡œ í´ë°± ì‹œë„
            try:
                return await _fetch_korean_stock_candles(symbol, "D", range_days)
            except Exception as e2:
                logger.error(f"í•œêµ­ ì£¼ì‹ ì¼ë´‰ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e2}")
                raise HTTPException(status_code=500, detail=f"í•œêµ­ ì£¼ì‹ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}")
    
    # ë¯¸êµ­ ì£¼ì‹ì¸ ê²½ìš°
    # yfinanceë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„ë´‰ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì‹œë„
    try:
        ticker = yf.Ticker(symbol.upper())
        
        # í•´ìƒë„ ë§¤í•‘
        period_map = {
            "1": "1d", "5": "1d", "15": "5d", "30": "5d",
            "60": "1mo", "120": "3mo", "240": "6mo",
            "D": _period_from_days(range_days), "W": "1y", "M": "2y"
        }
        period = period_map.get(resolution, "1mo")
        
        interval_map = {
            "1": "1m", "5": "5m", "15": "15m", "30": "30m",
            "60": "1h", "120": "2h", "240": "4h", 
            "D": "1d", "W": "1wk", "M": "1mo"
        }
        interval = interval_map.get(resolution, "1d")
        
        hist = ticker.history(period=period, interval=interval)
        
        if hist.empty:
            # ì¼ë´‰ìœ¼ë¡œ í´ë°±
            hist = ticker.history(period=_period_from_days(range_days), interval="1d")
        
        if hist.empty:
            raise HTTPException(status_code=404, detail=f"{symbol.upper()} ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ë°ì´í„° ë³€í™˜
        timestamps = []
        opens = []
        highs = []
        lows = []
        closes = []
        volumes = []
        
        for idx, row in hist.iterrows():
            ts = idx
            if isinstance(ts, pd.Timestamp):
                ts = ts.to_pydatetime()
            if ts.tzinfo is None:
                ts = ts.replace(tzinfo=dt.timezone.utc)
            
            timestamps.append(int(ts.timestamp()))
            opens.append(float(row["Open"]))
            highs.append(float(row["High"]))
            lows.append(float(row["Low"]))
            closes.append(float(row["Close"]))
            volumes.append(float(row["Volume"]))
        
        return CandleResponse(
            symbol=symbol.upper(),
            resolution=resolution,
            data=CandleSeries(
                timestamps=timestamps,
                opens=opens,
                highs=highs,
                lows=lows,
                closes=closes,
                volumes=volumes,
            ),
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë¯¸êµ­ ì£¼ì‹ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì°¨íŠ¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}")


async def _fetch_korean_stock_candles(symbol: str, resolution: str, range_days: int) -> CandleResponse:
    """
    í•œêµ­ ì£¼ì‹ ì°¨íŠ¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    FinanceDataReaderë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    try:
        # ì¢…ëª© ì½”ë“œ ì •ë¦¬ (005930.KS -> 005930)
        target_symbol = symbol.split('.')[0]
        
        # ê¸°ê°„ ì„¤ì •
        end_date = dt.datetime.now()
        # start_dateë¥¼ 0ì‹œ 0ë¶„ 0ì´ˆë¡œ ì„¤ì •í•˜ì—¬ í•´ë‹¹ ì¼ìì˜ ë°ì´í„°ë¥¼ í¬í•¨í•˜ë„ë¡ í•¨
        # range_daysê°€ ì‘ì„ ê²½ìš°(ì˜ˆ: 1ì¼), ì£¼ë§ì´ë‚˜ íœ´ì¼ì„ ê³ ë ¤í•˜ì—¬ ìµœì†Œ 7ì¼ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
        days_to_subtract = max(range_days, 7)
        start_date = (end_date - dt.timedelta(days=days_to_subtract)).replace(hour=0, minute=0, second=0, microsecond=0)
        
        # FinanceDataReader ë°ì´í„° ì¡°íšŒ
        # ì¼ë´‰ ë°ì´í„°ë§Œ ì œê³µë¨ (ë¶„ë´‰ì€ ì œí•œì )
        df = fdr.DataReader(target_symbol, start=start_date, end=end_date)
        
        if df is None or df.empty:
            raise HTTPException(status_code=404, detail=f"{symbol} ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
        # ë°ì´í„° ë³€í™˜
        # í•´ìƒë„ì— ë”°ë¥¸ ë¦¬ìƒ˜í”Œë§ (ì£¼ë´‰, ì›”ë´‰)
        if resolution == "W":
            df = df.resample('W').agg({
                'Open': 'first',
                'High': 'max',
                'Low': 'min',
                'Close': 'last',
                'Volume': 'sum'
            }).dropna()
        elif resolution == "M":
            df = df.resample('M').agg({
                'Open': 'first',
                'High': 'max',
                'Low': 'min',
                'Close': 'last',
                'Volume': 'sum'
            }).dropna()
        # ì—°ë´‰ì€ í•„ìš”ì‹œ ì¶”ê°€ (resolution == "Y")
        elif resolution == "Y":
             df = df.resample('Y').agg({
                'Open': 'first',
                'High': 'max',
                'Low': 'min',
                'Close': 'last',
                'Volume': 'sum'
            }).dropna()

        timestamps = []
        opens = []
        highs = []
        lows = []
        closes = []
        volumes = []
        
        for idx, row in df.iterrows():
            ts = idx
            if isinstance(ts, pd.Timestamp):
                ts = ts.to_pydatetime()
            
            # íƒ€ì„ì¡´ ì²˜ë¦¬ (UTCë¡œ í†µì¼)
            if ts.tzinfo is None:
                ts = ts.replace(tzinfo=dt.timezone.utc)
            
            timestamps.append(int(ts.timestamp()))
            opens.append(float(row["Open"]))
            highs.append(float(row["High"]))
            lows.append(float(row["Low"]))
            closes.append(float(row["Close"]))
            volumes.append(float(row["Volume"]))
            
        return CandleResponse(
            symbol=symbol,
            resolution=resolution,
            data=CandleSeries(
                timestamps=timestamps,
                opens=opens,
                highs=highs,
                lows=lows,
                closes=closes,
                volumes=volumes,
            ),
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"í•œêµ­ ì£¼ì‹ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"í•œêµ­ ì£¼ì‹ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {str(e)}")


@app.get("/healthz")
def health_check() -> dict[str, str]:
    return {"status": "ok"}


@app.on_event("startup")
async def _on_startup() -> None:
    global MARKET_REFRESH_TASK, NEWS_REFRESH_TASK
    MARKET_REFRESH_TASK = asyncio.create_task(_market_refresh_loop())
    NEWS_REFRESH_TASK = asyncio.create_task(_news_refresh_loop())
    asyncio.create_task(_refresh_market_cache_once())
    asyncio.create_task(_refresh_news_cache_once())


@app.on_event("shutdown")
async def _on_shutdown() -> None:
    tasks = [MARKET_REFRESH_TASK, NEWS_REFRESH_TASK]
    for task in tasks:
        if task:
            task.cancel()
    for task in tasks:
        if task:
            with contextlib.suppress(asyncio.CancelledError):
                await task


def _fallback_quote_yfinance(
    provider_symbol: str, display_symbol: str, display_name: Optional[str]
) -> MarketQuote:
    cache_entry = QUOTE_CACHE.get(provider_symbol.upper())
    if cache_entry and time.time() - cache_entry[1] < CACHE_TTL_SECONDS:
        return cache_entry[0]

    try:
        df = _yf_download_with_retry(provider_symbol, "5d", "1d")
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=404, detail=f"{display_symbol} ì‹œì„¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.") from exc

    if df.empty:
        raise HTTPException(status_code=404, detail=f"{display_symbol} ì‹œì„¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    df = df.dropna()
    if df.empty:
        raise HTTPException(status_code=404, detail=f"{display_symbol} ì‹œì„¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    last = df.iloc[-1]
    prev = df.iloc[-2] if len(df) > 1 else last

    prev_close = float(prev["Close"]) if not pd.isna(prev["Close"]) else None
    current = float(last["Close"])

    if prev_close and prev_close != 0:
        change = current - prev_close
        percent = (change / prev_close) * 100
    else:
        change = 0.0
        percent = 0.0

    timestamp = df.index[-1]
    if isinstance(timestamp, pd.Timestamp):
        timestamp = timestamp.to_pydatetime()
    if timestamp.tzinfo is None:
        timestamp = timestamp.replace(tzinfo=dt.timezone.utc)

    quote = MarketQuote(
        symbol=display_symbol,
        name=display_name or display_symbol,
        current=current,
        change=change,
        percent=percent,
        high=float(last["High"]) if not pd.isna(last["High"]) else None,
        low=float(last["Low"]) if not pd.isna(last["Low"]) else None,
        open=float(last["Open"]) if not pd.isna(last["Open"]) else None,
        previous_close=prev_close,
        timestamp=timestamp,
    )
    QUOTE_CACHE[provider_symbol.upper()] = (quote, time.time())
    return quote




def _fallback_quote_yfinance(
    provider_symbol: str, display_symbol: str, display_name: Optional[str]
) -> MarketQuote:
    # í•œêµ­ ì£¼ì‹ì¸ ê²½ìš° FinanceDataReader ì‚¬ìš©
    if provider_symbol.endswith(".KS") or provider_symbol.endswith(".KQ") or (provider_symbol.isdigit() and len(provider_symbol) == 6):
        return _fetch_korean_stock_quote(provider_symbol, display_name)

    cache_entry = QUOTE_CACHE.get(provider_symbol.upper())
    if cache_entry and time.time() - cache_entry[1] < CACHE_TTL_SECONDS:
        return cache_entry[0]

    try:
        df = _yf_download_with_retry(provider_symbol, "5d", "1d")
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=404, detail=f"{display_symbol} ì‹œì„¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.") from exc

    if df.empty:
        raise HTTPException(status_code=404, detail=f"{display_symbol} ì‹œì„¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    df = df.dropna()
    if df.empty:
        raise HTTPException(status_code=404, detail=f"{display_symbol} ì‹œì„¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    last = df.iloc[-1]
    prev = df.iloc[-2] if len(df) > 1 else last

    prev_close = float(prev["Close"]) if not pd.isna(prev["Close"]) else None
    current = float(last["Close"])

    if prev_close and prev_close != 0:
        change = current - prev_close
        percent = (change / prev_close) * 100
    else:
        change = 0.0
        percent = 0.0

    timestamp = df.index[-1]
    if isinstance(timestamp, pd.Timestamp):
        timestamp = timestamp.to_pydatetime()
    if timestamp.tzinfo is None:
        timestamp = timestamp.replace(tzinfo=dt.timezone.utc)

    quote = MarketQuote(
        symbol=display_symbol,
        name=display_name or display_symbol,
        current=current,
        change=change,
        percent=percent,
        high=float(last["High"]) if not pd.isna(last["High"]) else None,
        low=float(last["Low"]) if not pd.isna(last["Low"]) else None,
        open=float(last["Open"]) if not pd.isna(last["Open"]) else None,
        previous_close=prev_close,
        timestamp=timestamp,
    )
    QUOTE_CACHE[provider_symbol.upper()] = (quote, time.time())
    return quote


def _map_interval_and_period(resolution: str, range_days: int) -> tuple[str, str]:
    res = resolution.upper()
    if res == "1":
        return "1m", "5d"
    if res == "5":
        return "5m", f"{min(range_days, 30)}d"
    if res == "15":
        return "15m", f"{min(range_days, 30)}d"
    if res == "30":
        return "30m", f"{min(range_days, 60)}d"
    if res == "60":
        return "60m", f"{min(range_days, 60)}d"
    if res == "240":
        return "1h", _period_from_days(range_days)
    if res in {"D", "1D"}:
        return "1d", _period_from_days(range_days)
    if res in {"W", "1W"}:
        return "1wk", _period_from_days(range_days)
    if res in {"M", "1M"}:
        return "1mo", _period_from_days(range_days)
    return "1d", _period_from_days(range_days)


def _period_from_days(days: int) -> str:
    if days <= 5:
        return "5d"
    if days <= 30:
        return "1mo"
    if days <= 90:
        return "3mo"
    if days <= 180:
        return "6mo"
    if days <= 365:
        return "1y"
    if days <= 730:
        return "2y"
    if days <= 1825:
        return "5y"
    return "10y"


def _fallback_candles_yfinance(
    provider_symbol: str, display_symbol: str, resolution: str, range_days: int
) -> tuple[List[dict], CandleResponse]:
    cache_key = (provider_symbol.upper(), resolution, range_days)
    cache_entry = CANDLE_CACHE.get(cache_key)
    if cache_entry and time.time() - cache_entry[1] < CACHE_TTL_SECONDS:
        candles = cache_entry[0]
        records = [
            {
                "date": dt.datetime.fromtimestamp(ts, tz=dt.timezone.utc),
                "open": opens,
                "high": highs,
                "low": lows,
                "close": closes,
                "volume": volumes,
            }
            for ts, opens, highs, lows, closes, volumes in zip(
                candles.data.timestamps,
                candles.data.opens,
                candles.data.highs,
                candles.data.lows,
                candles.data.closes,
                candles.data.volumes,
            )
        ]
        records.sort(key=lambda item: item["date"], reverse=True)
        return records, candles

    primary_interval, primary_period = _map_interval_and_period(resolution, range_days)
    fallback_candidates: List[tuple[str, str]] = [
        (primary_interval, primary_period),
    ]

    if primary_interval not in {"1d", "1wk", "1mo"}:
        fallback_candidates.append(("1d", _period_from_days(range_days)))
    fallback_candidates.append(("1wk", _period_from_days(max(range_days, 30))))

    last_error: Optional[Exception] = None

    for interval, period in fallback_candidates:
        try:
            df = _yf_download_with_retry(provider_symbol, period, interval)
        except Exception as exc:  # noqa: BLE001
            last_error = exc
            continue

        if df.empty:
            last_error = ValueError("empty dataframe")
            continue

        df = df.dropna()
        if df.empty:
            last_error = ValueError("empty dataframe after dropna")
            continue

        timestamps: List[int] = []
        for idx in df.index:
            ts = idx.to_pydatetime() if isinstance(idx, pd.Timestamp) else pd.Timestamp(idx).to_pydatetime()
            if ts.tzinfo is None:
                ts = ts.replace(tzinfo=dt.timezone.utc)
            timestamps.append(int(ts.timestamp()))

        records = []
        for idx, ts_value in zip(df.index, timestamps):
            ts_dt = dt.datetime.fromtimestamp(ts_value, tz=dt.timezone.utc)
            row = df.loc[idx]
            records.append(
                {
                    "date": ts_dt,
                    "open": float(row["Open"]),
                    "high": float(row["High"]),
                    "low": float(row["Low"]),
                    "close": float(row["Close"]),
                    "volume": float(row["Volume"]),
                }
            )

        records.sort(key=lambda item: item["date"], reverse=True)
        candles = _candles_from_series(display_symbol, records, range_days, resolution)
        CANDLE_CACHE[cache_key] = (candles, time.time())
        return records, candles

    detail = f"{display_symbol} ì°¨íŠ¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    if last_error:
        detail = f"{detail} (fallback ì‹¤íŒ¨: {last_error})"
    raise HTTPException(status_code=404, detail=detail)


def _yf_download_with_retry(symbol: str, period: str, interval: str, attempts: int = 3) -> pd.DataFrame:
    delay = 1.0
    last_exc: Optional[Exception] = None

    for _ in range(attempts):
        try:
            df = yf.download(
                symbol,
                period=period,
                interval=interval,
                progress=False,
                auto_adjust=False,
                threads=False,
            )
            return df
        except Exception as exc:  # noqa: BLE001
            last_exc = exc
            time.sleep(delay)
            delay *= 2

    if last_exc:
        raise last_exc
    return pd.DataFrame()

