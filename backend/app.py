from __future__ import annotations

import asyncio
import datetime as dt
import os
import time
from typing import Dict, List, Optional, Tuple

import contextlib

import logging

import httpx
import pandas as pd
import yfinance as yf
import feedparser
from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

from services.ai import rank_recommendations, summarize_headline, translate_to_korean

logger = logging.getLogger(__name__)

# Ollama í´ë¼ì´ì–¸íŠ¸ (ì„ íƒì )
try:
    import ollama
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False
    logger.warning("ollama not available, chatbot will use fallback")

app = FastAPI(
    title="Breaking Share AI API",
    description="AI-powered helpers for the Breaking Share homepage.",
    version="0.1.0",
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


class SummarizeRequest(BaseModel):
    text: str = Field(..., description="ë‰´ìŠ¤ ì „ë¬¸ ë˜ëŠ” ìš”ì•½í•˜ê³  ì‹¶ì€ í•œê¸€ ë¬¸ìž¥")
    max_tokens: Optional[int] = Field(
        180,
        ge=32,
        le=512,
        description="ìƒì„± ìš”ì•½ì˜ ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸ 180)",
    )


class SummarizeResponse(BaseModel):
    summary: str


class ChatRequest(BaseModel):
    message: str = Field(..., description="ì‚¬ìš©ìž ë©”ì‹œì§€")
    include_market: bool = Field(True, description="ì‹œìž¥ ë°ì´í„° í¬í•¨ ì—¬ë¶€")
    include_news: bool = Field(True, description="ë‰´ìŠ¤ ë°ì´í„° í¬í•¨ ì—¬ë¶€")
    max_news: int = Field(3, ge=0, le=10, description="í¬í•¨í•  ìµœëŒ€ ë‰´ìŠ¤ ê°œìˆ˜")


class ChatSource(BaseModel):
    type: str  # "news", "market", "local_llm"
    title: Optional[str] = None
    url: Optional[str] = None
    content: Optional[str] = None


class ChatResponse(BaseModel):
    reply: str
    sources: List[ChatSource] = Field(default_factory=list)


class RecommendationRequest(BaseModel):
    tickers: List[str] = Field(
        ...,
        min_length=1,
        description="ìŠ¤ì½”ì–´ë§ì„ ì§„í–‰í•  í‹°ì»¤ ëª©ë¡ (ì˜ˆ: ['NVDA', 'AAPL'])",
    )
    weights: Optional[dict[str, float]] = Field(
        None,
        description="ìš”ì†Œë³„ ê°€ì¤‘ì¹˜. eps_growth, revenue_growth, momentum, volatility ì¤‘ ì¼ë¶€/ì „ë¶€ ì§€ì • ê°€ëŠ¥",
    )


class RecommendationItem(BaseModel):
    ticker: str
    composite_score: float
    eps_growth: float
    revenue_growth: float
    momentum: float
    volatility: float


class RecommendationResponse(BaseModel):
    generated_at: dt.datetime
    items: List[RecommendationItem]


class NewsArticle(BaseModel):
    headline: str
    headline_ko: Optional[str] = None
    summary: Optional[str] = None
    summary_ko: Optional[str] = None
    url: str
    source: Optional[str] = None
    published_at: dt.datetime
    symbols: List[str] = Field(default_factory=list)
    image: Optional[str] = None


class MarketQuote(BaseModel):
    symbol: str
    name: str
    current: float
    change: float
    percent: float
    high: Optional[float] = None
    low: Optional[float] = None
    open: Optional[float] = None
    previous_close: Optional[float] = None
    timestamp: dt.datetime


class SymbolSearchResult(BaseModel):
    symbol: str
    description: str
    type: Optional[str] = None
    exchange: Optional[str] = None


class CandleSeries(BaseModel):
    timestamps: List[int]
    opens: List[float]
    highs: List[float]
    lows: List[float]
    closes: List[float]
    volumes: List[float]


class CandleResponse(BaseModel):
    symbol: str
    resolution: str
    data: CandleSeries


@app.post("/api/summarize", response_model=SummarizeResponse)
def summarize_news(payload: SummarizeRequest) -> SummarizeResponse:
    try:
        summary = summarize_headline(payload.text, max_tokens=payload.max_tokens)
        return SummarizeResponse(summary=summary)
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=500, detail=str(exc)) from exc


@app.post("/api/chat", response_model=ChatResponse)
async def chat_with_llm(payload: ChatRequest) -> ChatResponse:
    """
    LLM API (Ollama)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì±—ë´‡ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    Local LLM (transformers)ì„ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ìš”ì•½ë„ í•¨ê»˜ ì œê³µí•©ë‹ˆë‹¤.
    """
    sources: List[ChatSource] = []
    context_parts = []
    
    # 1. ì‹œìž¥ ë°ì´í„° ìˆ˜ì§‘ (ìš”ì²­ëœ ê²½ìš°)
    market_info = ""
    if payload.include_market:
        try:
            async with MARKET_CACHE_LOCK:
                market_quotes = []
                for symbol, name in MARKET_OVERVIEW_SYMBOLS[:3]:  # ì£¼ìš” ì§€ìˆ˜ 3ê°œë§Œ
                    entry = MARKET_CACHE.get(symbol.upper())
                    if entry:
                        quote = entry["quote"]  # type: ignore[index]
                        market_quotes.append(f"{name}: {quote.current:.2f} ({quote.percent:+.2f}%)")
            
            if market_quotes:
                market_info = "ì£¼ìš” ì‹œìž¥ ì§€ìˆ˜:\n" + "\n".join(market_quotes)
                sources.append(ChatSource(
                    type="market",
                    title="ì‹œìž¥ ë°ì´í„°",
                    content=market_info
                ))
        except Exception as e:
            logger.warning(f"ì‹œìž¥ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    # 2. ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ (ìš”ì²­ëœ ê²½ìš°) - AI ë¶„ì„ ì§ˆë¬¸ì¼ ë•ŒëŠ” ìƒëžµí•˜ì—¬ ì†ë„ ê°œì„ 
    news_info = ""
    user_message = payload.message  # ë¨¼ì € ì •ì˜
    is_ai_analysis_question = "ì†ì ˆ" in user_message or "ìµì ˆ" in user_message or "AI ë¶„ì„" in user_message or "ë°˜ë³µ" in user_message
    
    if payload.include_news and not is_ai_analysis_question:
        try:
            articles_list = await _fetch_usa_news()
            articles = articles_list[:payload.max_news] if isinstance(articles_list, list) else list(articles_list)[:payload.max_news]
            if articles:
                news_items = []
                for article in articles:
                    headline = article.headline_ko or article.headline
                    # Local LLMì„ ì‚¬ìš©í•˜ì—¬ ìš”ì•½ (transformers) - íƒ€ìž„ì•„ì›ƒ ì„¤ì •
                    summary = article.summary_ko or article.summary
                    if summary:
                        try:
                            # Local LLMìœ¼ë¡œ ìš”ì•½ ìƒì„± (ë¹ ë¥¸ ì‘ë‹µì„ ìœ„í•´ ì§§ê²Œ)
                            summary_short = summarize_headline(summary, max_tokens=30)
                            news_items.append(f"- {headline}: {summary_short}")
                        except Exception:
                            news_items.append(f"- {headline}")
                    else:
                        news_items.append(f"- {headline}")
                    
                    sources.append(ChatSource(
                        type="news",
                        title=headline,
                        url=article.url,
                        content=summary or headline
                    ))
                
                news_info = "ìµœì‹  ê²½ì œ ë‰´ìŠ¤:\n" + "\n".join(news_items)
                sources.append(ChatSource(
                    type="local_llm",
                    title="Local LLM ìš”ì•½",
                    content="transformersë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ìš”ì•½ ìƒì„±"
                ))
        except Exception as e:
            logger.warning(f"ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    # 3. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context = ""
    if market_info:
        context += market_info + "\n\n"
    if news_info:
        context += news_info + "\n\n"
    
    # 4. LLM API (Ollama)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±
    # user_messageëŠ” ì´ë¯¸ ìœ„ì—ì„œ ì •ì˜ë¨
    
    # AI ë¶„ì„ ê¸°ëŠ¥ì— ëŒ€í•œ ì§ˆë¬¸ì¸ì§€ í™•ì¸ (ì´ë¯¸ ìœ„ì—ì„œ í™•ì¸í•¨)
    ai_analysis_context = ""
    if is_ai_analysis_question:
        ai_analysis_context = """
        
TradeNoteì˜ AI ë¶„ì„ ê¸°ëŠ¥ì— ëŒ€í•œ ì •ë³´:
1. "ì†ì ˆ ì‹œ ë°˜ë³µë˜ëŠ” ë¬¸ì œì  ì°¾ê¸°" ê¸°ëŠ¥:
   - ë§¤ë§¤ì¼ì§€ì—ì„œ ì†ì ˆí•œ ê±°ëž˜ì˜ 'ì†ì ˆí•œ ì´ìœ 'ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤
   - ìžì£¼ ë°˜ë³µë˜ëŠ” íŒ¨í„´ê³¼ ë¬¸ì œì ì„ ì°¾ì•„ë“œë¦½ë‹ˆë‹¤
   - ìµœì†Œ ë§¤ë§¤ì¼ì§€ê°€ 1ê°œ ì´ìƒ ìžˆì–´ì•¼ ìž‘ë™í•©ë‹ˆë‹¤
   - ì†ì ˆ ì‚¬ìœ ë¥¼ ê¸°ë¡í•˜ë©´ ë” ì •í™•í•œ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤
   
2. "ìµì ˆ ì‹œ ë°˜ë³µë˜ëŠ” ì¢‹ì€ ìŠµê´€ ì°¾ê¸°" ê¸°ëŠ¥:
   - ë§¤ë§¤ì¼ì§€ì—ì„œ ìµì ˆí•œ ê±°ëž˜ì˜ 'ìµì ˆí•œ ì´ìœ 'ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤
   - ë°˜ë³µë˜ëŠ” ì¢‹ì€ ìŠµê´€ê³¼ íŒ¨í„´ì„ ì°¾ì•„ë“œë¦½ë‹ˆë‹¤
   - ìµœì†Œ ë§¤ë§¤ì¼ì§€ê°€ 1ê°œ ì´ìƒ ìžˆì–´ì•¼ ìž‘ë™í•©ë‹ˆë‹¤
   - ìµì ˆ ì‚¬ìœ ë¥¼ ê¸°ë¡í•˜ë©´ ë” ì •í™•í•œ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤

ì´ ê¸°ëŠ¥ë“¤ì€ ì‚¬ìš©ìžì˜ ë§¤ë§¤ íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ ê°œì„ ì ì„ ì°¾ì•„ì£¼ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤.
"""
    
    system_prompt = """ë‹¹ì‹ ì€ TradeNoteì˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ìž…ë‹ˆë‹¤. ì£¼ì‹ ì‹œìž¥, ê²½ì œ ë‰´ìŠ¤, ê·¸ë¦¬ê³  TradeNoteì˜ AI ë¶„ì„ ê¸°ëŠ¥ì— ëŒ€í•´ ë„ì›€ì„ ì£¼ëŠ” ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.
ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
ì œê³µëœ ì‹œìž¥ ë°ì´í„°ì™€ ë‰´ìŠ¤ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
AI ë¶„ì„ ê¸°ëŠ¥ì— ëŒ€í•œ ì§ˆë¬¸ì´ ìžˆìœ¼ë©´ ìƒì„¸í•˜ê³  ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”."""
    
    prompt = f"{system_prompt}{ai_analysis_context}\n\n{context}ì‚¬ìš©ìž ì§ˆë¬¸: {user_message}\n\në‹µë³€:"
    
    reply = ""
    try:
        if OLLAMA_AVAILABLE:
            # Ollama API ì‚¬ìš© (ë¡œì»¬ LLM API)
            ollama_model = os.getenv("OLLAMA_MODEL", "qwen2.5:0.5b")  # ê¸°ë³¸ê°’: ìž‘ì€ ëª¨ë¸
            try:
                # httpxë¥¼ ì‚¬ìš©í•˜ì—¬ Ollama API ì§ì ‘ í˜¸ì¶œ (ë” ì•ˆì •ì )
                ollama_url = os.getenv("OLLAMA_HOST", "http://127.0.0.1:11434")
                # íƒ€ìž„ì•„ì›ƒì„ 120ì´ˆë¡œ ëŠ˜ë¦¼ (ëª¨ë¸ ë¡œë”© ì‹œê°„ í¬í•¨)
                async with httpx.AsyncClient(timeout=120.0) as client:
                    response = await client.post(
                        f"{ollama_url}/api/chat",
                        json={
                            "model": ollama_model,
                            "messages": [
                                {"role": "system", "content": system_prompt + ai_analysis_context},
                                {"role": "user", "content": f"{context}ì§ˆë¬¸: {user_message}" if context else f"ì§ˆë¬¸: {user_message}"}
                            ],
                            "options": {
                                "temperature": 0.7,
                                "num_predict": 200 if is_ai_analysis_question else 300,
                            },
                            "stream": False
                        }
                    )
                    if response.status_code == 200:
                        data = response.json()
                        reply = data.get("message", {}).get("content", "")
                        if not reply or len(reply.strip()) == 0:
                            raise ValueError("Ollama ì‘ë‹µì´ ë¹„ì–´ìžˆìŠµë‹ˆë‹¤")
                        sources.append(ChatSource(
                            type="local_llm",
                            title="Ollama LLM API",
                            content=f"ëª¨ë¸: {ollama_model}"
                        ))
                        logger.info(f"Ollama API ì„±ê³µ: {len(reply)}ìž ì‘ë‹µ ìƒì„±")
                    else:
                        raise HTTPException(status_code=response.status_code, detail=f"Ollama API ì˜¤ë¥˜: {response.text}")
            except httpx.TimeoutException:
                logger.warning("Ollama API íƒ€ìž„ì•„ì›ƒ, fallback ì‚¬ìš©")
                reply = _generate_fallback_reply(user_message, market_info, news_info)
            except Exception as e:
                logger.warning(f"Ollama API í˜¸ì¶œ ì‹¤íŒ¨: {e}, fallback ì‚¬ìš©")
                reply = _generate_fallback_reply(user_message, market_info, news_info)
        else:
            # Ollamaê°€ ì—†ìœ¼ë©´ ì¦‰ì‹œ fallback ì‚¬ìš©
            reply = _generate_fallback_reply(user_message, market_info, news_info)
    except Exception as e:
        logger.error(f"LLM ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
        reply = _generate_fallback_reply(user_message, market_info, news_info)
    
    if not reply:
        reply = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    return ChatResponse(reply=reply, sources=sources)


def _generate_fallback_reply(message: str, market_info: str, news_info: str) -> str:
    """Ollamaê°€ ì—†ì„ ë•Œ ì‚¬ìš©í•˜ëŠ” ë¹ ë¥¸ fallback ì‘ë‹µ"""
    # ê°„ë‹¨í•œ ì¸ì‚¬ë§ ì²˜ë¦¬
    message_lower = message.lower().strip()
    if message_lower in ["ì•ˆë…•", "ì•ˆë…•í•˜ì„¸ìš”", "hi", "hello", "ì•ˆë…•í•˜ì„¸ìš”!", "ì•ˆë…•!"]:
        return "ì•ˆë…•í•˜ì„¸ìš”! TradeNote AI ì–´ì‹œìŠ¤í„´íŠ¸ìž…ë‹ˆë‹¤. ì£¼ì‹ ì‹œìž¥, ê²½ì œ ë‰´ìŠ¤, ê·¸ë¦¬ê³  AI ë¶„ì„ ê¸°ëŠ¥ì— ëŒ€í•´ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
    
    reply_parts = []
    
    # AI ë¶„ì„ ê¸°ëŠ¥ì— ëŒ€í•œ ì§ˆë¬¸ ì²˜ë¦¬ (ê°€ìž¥ ë¹ ë¥´ê²Œ ì‘ë‹µ)
    if "ì†ì ˆ" in message or "ìµì ˆ" in message or "AI ë¶„ì„" in message or "ë°˜ë³µ" in message or "ë¬¸ì œì " in message or "ìŠµê´€" in message:
        if "ì†ì ˆ" in message or "ë¬¸ì œì " in message:
            reply_parts.append("""ì†ì ˆ ì‹œ ë°˜ë³µë˜ëŠ” ë¬¸ì œì  ì°¾ê¸° ê¸°ëŠ¥ì— ëŒ€í•´ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

ðŸ“Š **ê¸°ëŠ¥ ì„¤ëª…:**
ì´ ê¸°ëŠ¥ì€ ë§¤ë§¤ì¼ì§€ì—ì„œ ì†ì ˆí•œ ê±°ëž˜ì˜ 'ì†ì ˆí•œ ì´ìœ 'ë¥¼ ë¶„ì„í•˜ì—¬ ìžì£¼ ë°˜ë³µë˜ëŠ” íŒ¨í„´ê³¼ ë¬¸ì œì ì„ ì°¾ì•„ë“œë¦½ë‹ˆë‹¤.

ðŸ”§ **ì‚¬ìš© ë°©ë²•:**
1. ë§¤ë§¤ì¼ì§€ì—ì„œ ì†ì ˆ ê±°ëž˜ë¥¼ ê¸°ë¡í•˜ì„¸ìš”
2. ê° ê±°ëž˜ì˜ 'ì†ì ˆí•œ ì´ìœ 'ë¥¼ ìƒì„¸ížˆ ê¸°ë¡í•˜ì„¸ìš”
3. AI ë¶„ì„ íŽ˜ì´ì§€ì—ì„œ 'ì†ì ˆ ì‹œ ë°˜ë³µë˜ëŠ” ë¬¸ì œì  ì°¾ê¸°' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”

ðŸ“ **í•„ìš”í•œ ë°ì´í„°:**
- ìµœì†Œ ë§¤ë§¤ì¼ì§€ 1ê°œ ì´ìƒ
- ì†ì ˆ ê±°ëž˜ì˜ ì†ì ˆ ì‚¬ìœ  ê¸°ë¡

ðŸ’¡ **í™œìš© ë°©ë²•:**
ë°˜ë³µë˜ëŠ” ë¬¸ì œì ì„ ë°œê²¬í•˜ë©´, í•´ë‹¹ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ êµ¬ì²´ì ì¸ í–‰ë™ ê³„íšì„ ìˆ˜ë¦½í•˜ì„¸ìš”. ë§¤ë§¤ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ì–´ ì‹¤ìˆ˜ë¥¼ ë°©ì§€í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.""")
        elif "ìµì ˆ" in message or "ìŠµê´€" in message:
            reply_parts.append("""ìµì ˆ ì‹œ ë°˜ë³µë˜ëŠ” ì¢‹ì€ ìŠµê´€ ì°¾ê¸° ê¸°ëŠ¥ì— ëŒ€í•´ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

ðŸ“Š **ê¸°ëŠ¥ ì„¤ëª…:**
ì´ ê¸°ëŠ¥ì€ ë§¤ë§¤ì¼ì§€ì—ì„œ ìµì ˆí•œ ê±°ëž˜ì˜ 'ìµì ˆí•œ ì´ìœ 'ë¥¼ ë¶„ì„í•˜ì—¬ ë°˜ë³µë˜ëŠ” ì¢‹ì€ ìŠµê´€ê³¼ íŒ¨í„´ì„ ì°¾ì•„ë“œë¦½ë‹ˆë‹¤.

ðŸ”§ **ì‚¬ìš© ë°©ë²•:**
1. ë§¤ë§¤ì¼ì§€ì—ì„œ ìµì ˆ ê±°ëž˜ë¥¼ ê¸°ë¡í•˜ì„¸ìš”
2. ê° ê±°ëž˜ì˜ 'ìµì ˆí•œ ì´ìœ 'ë¥¼ ìƒì„¸ížˆ ê¸°ë¡í•˜ì„¸ìš”
3. AI ë¶„ì„ íŽ˜ì´ì§€ì—ì„œ 'ìµì ˆ ì‹œ ë°˜ë³µë˜ëŠ” ì¢‹ì€ ìŠµê´€ ì°¾ê¸°' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”

ðŸ“ **í•„ìš”í•œ ë°ì´í„°:**
- ìµœì†Œ ë§¤ë§¤ì¼ì§€ 1ê°œ ì´ìƒ
- ìµì ˆ ê±°ëž˜ì˜ ìµì ˆ ì‚¬ìœ  ê¸°ë¡

ðŸ’¡ **í™œìš© ë°©ë²•:**
ë°˜ë³µë˜ëŠ” ì¢‹ì€ ìŠµê´€ì„ ë°œê²¬í•˜ë©´, ì´ë¥¼ ë”ìš± ì²´ê³„í™”í•˜ê³  ì¼ê´€ë˜ê²Œ ì ìš©í•˜ì„¸ìš”. ì„±ê³µ íŒ¨í„´ì„ ê°•í™”í•˜ì—¬ ìŠ¹ë¥ ì„ ë†’ì¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.""")
        else:
            reply_parts.append("""AI ë¶„ì„ ê¸°ëŠ¥ì— ëŒ€í•´ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

TradeNoteì˜ AI ë¶„ì„ ê¸°ëŠ¥ì€ ë‘ ê°€ì§€ê°€ ìžˆìŠµë‹ˆë‹¤:

1ï¸âƒ£ **ì†ì ˆ ì‹œ ë°˜ë³µë˜ëŠ” ë¬¸ì œì  ì°¾ê¸°**
   - ì†ì ˆ ê±°ëž˜ì˜ íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ ê°œì„ ì ì„ ì°¾ìŠµë‹ˆë‹¤
   - ë§¤ë§¤ì¼ì§€ì—ì„œ ì†ì ˆ ì‚¬ìœ ë¥¼ ê¸°ë¡í•˜ë©´ ë” ì •í™•í•œ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤

2ï¸âƒ£ **ìµì ˆ ì‹œ ë°˜ë³µë˜ëŠ” ì¢‹ì€ ìŠµê´€ ì°¾ê¸°**
   - ìµì ˆ ê±°ëž˜ì˜ íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ ì„±ê³µ ìš”ì¸ì„ ì°¾ìŠµë‹ˆë‹¤
   - ë§¤ë§¤ì¼ì§€ì—ì„œ ìµì ˆ ì‚¬ìœ ë¥¼ ê¸°ë¡í•˜ë©´ ë” ì •í™•í•œ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤

ðŸ’¡ **íŒ:** ë” ìžì„¸í•œ ì •ë³´ë¥¼ ì›í•˜ì‹œë©´ ê° ê¸°ëŠ¥ì˜ ë„ì›€ë§ ë²„íŠ¼(ðŸ’¬)ì„ í´ë¦­í•˜ì„¸ìš”.""")
        return "\n\n".join(reply_parts)
    
    # ì‹œìž¥ ë°ì´í„° ê´€ë ¨ ì§ˆë¬¸
    if "ì‹œìž¥" in message or "ì§€ìˆ˜" in message or "ì£¼ê°€" in message:
        if market_info:
            reply_parts.append(market_info)
        else:
            reply_parts.append("ì‹œìž¥ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # ë‰´ìŠ¤ ê´€ë ¨ ì§ˆë¬¸
    if "ë‰´ìŠ¤" in message or "ì†Œì‹" in message:
        if news_info:
            reply_parts.append(news_info)
        else:
            reply_parts.append("ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # ê¸°ë³¸ ì‘ë‹µ - ë” ìžì—°ìŠ¤ëŸ½ê²Œ ê°œì„ 
    if not reply_parts:
        # ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ ì‘ë‹µ
        if any(word in message for word in ["ì‹œìž¥", "ì£¼ê°€", "ì§€ìˆ˜", "ì£¼ì‹"]):
            reply_parts.append("ì‹œìž¥ ì •ë³´ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì…¨ë„¤ìš”. í˜„ìž¬ ì‹œìž¥ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ ì •í™•í•œ ë‹µë³€ì„ ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤.")
            reply_parts.append("ì‹œìž¥ ë°ì´í„° ê¸°ëŠ¥ì´ í™œì„±í™”ë˜ë©´ ë” ìžì„¸í•œ ì •ë³´ë¥¼ ì œê³µí•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.")
        elif any(word in message for word in ["ë‰´ìŠ¤", "ì†Œì‹", "ì´ìŠˆ"]):
            reply_parts.append("ë‰´ìŠ¤ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì…¨ë„¤ìš”. í˜„ìž¬ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ ìµœì‹  ì •ë³´ë¥¼ ì œê³µí•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.")
            reply_parts.append("ë‰´ìŠ¤ ê¸°ëŠ¥ì´ í™œì„±í™”ë˜ë©´ ìµœì‹  ê²½ì œ ë‰´ìŠ¤ë¥¼ ì œê³µí•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.")
        else:
            reply_parts.append(f"'{message}'ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì…¨ë„¤ìš”.")
            reply_parts.append("ë‹¤ìŒ ì£¼ì œì— ëŒ€í•´ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìžˆìŠµë‹ˆë‹¤:")
            reply_parts.append("â€¢ AI ë¶„ì„ ê¸°ëŠ¥ (ì†ì ˆ/ìµì ˆ íŒ¨í„´ ë¶„ì„)")
            reply_parts.append("â€¢ ì£¼ì‹ ì‹œìž¥ ì •ë³´")
            reply_parts.append("â€¢ ê²½ì œ ë‰´ìŠ¤")
            reply_parts.append("ì›í•˜ì‹œëŠ” ì£¼ì œë¥¼ ì„ íƒí•´ì£¼ì‹œë©´ ë” ìžì„¸ížˆ ì„¤ëª…í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.")
    
    return "\n\n".join(reply_parts)


@app.post("/api/recommendations", response_model=RecommendationResponse)
def generate_recommendations(payload: RecommendationRequest) -> RecommendationResponse:
    try:
        history = yf.download(
            payload.tickers,
            period="6mo",
            interval="1d",
            group_by="ticker",
            auto_adjust=True,
            progress=False,
        )
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=502, detail=f"ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {exc}") from exc

    if history.empty:
        raise HTTPException(status_code=404, detail="ë‹¤ìš´ë¡œë“œí•œ ì‹œì„¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    metrics = []
    for ticker in payload.tickers:
        try:
            ticker_history = history[ticker] if len(payload.tickers) > 1 else history
            close_prices = ticker_history["Close"].dropna()
            if close_prices.empty:
                continue

            returns = close_prices.pct_change().dropna()
            momentum = (close_prices.iloc[-1] / close_prices.iloc[0]) - 1
            volatility = returns.std()

            earnings = yf.Ticker(ticker).get_earnings_dates(limit=4)
            revenue_growth = 0.0
            eps_growth = 0.0
            if earnings is not None and hasattr(earnings, 'empty') and not earnings.empty:
                earnings = earnings.sort_index()
                if "Revenue" in earnings.columns and len(earnings["Revenue"].dropna()) >= 2:
                    revenue_growth = (
                        earnings["Revenue"].iloc[-1] - earnings["Revenue"].iloc[-2]
                    ) / abs(earnings["Revenue"].iloc[-2])
                if "EPS" in earnings.columns and len(earnings["EPS"].dropna()) >= 2:
                    eps_growth = (
                        earnings["EPS"].iloc[-1] - earnings["EPS"].iloc[-2]
                    ) / abs(earnings["EPS"].iloc[-2])

            metrics.append(
                {
                    "ticker": ticker,
                    "momentum": float(momentum),
                    "volatility": float(volatility),
                    "revenue_growth": float(revenue_growth),
                    "eps_growth": float(eps_growth),
                }
            )
        except Exception as exc:  # noqa: BLE001
            raise HTTPException(status_code=500, detail=f"{ticker} ë¶„ì„ ì‹¤íŒ¨: {exc}") from exc

    if not metrics:
        raise HTTPException(status_code=404, detail="í‰ê°€ ê°€ëŠ¥í•œ ì¢…ëª© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    default_weights = {
        "momentum": 0.4,
        "volatility": -0.2,  # ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
        "revenue_growth": 0.2,
        "eps_growth": 0.2,
    }

    if payload.weights:
        default_weights.update(payload.weights)

    ranked = rank_recommendations(metrics, weights=default_weights)
    return RecommendationResponse(
        generated_at=dt.datetime.utcnow(),
        items=[RecommendationItem(**item) for item in ranked],
    )


FINNHUB_NEWS_URL = "https://finnhub.io/api/v1/news"
FINNHUB_QUOTE_URL = "https://finnhub.io/api/v1/quote"
FINNHUB_SEARCH_URL = "https://finnhub.io/api/v1/search"
FINNHUB_CANDLE_URL = "https://finnhub.io/api/v1/stock/candle"

MARKET_OVERVIEW_SYMBOLS = [
    ("SPY", "S&P 500 ETF"),
    ("QQQ", "NASDAQ 100 ETF"),
    ("DIA", "Dow Jones 30 ETF"),
    ("IWM", "Russell 2000 ETF"),
    ("XLF", "Financial Select Sector"),
    ("XLE", "Energy Select Sector"),
    ("XLK", "Technology Select Sector"),
]

CACHE_TTL_SECONDS = 300
QUOTE_CACHE: Dict[str, tuple[MarketQuote, float]] = {}
CANDLE_CACHE: Dict[Tuple[str, str, int], tuple[CandleResponse, float]] = {}
ALPHAVANTAGE_URL = "https://www.alphavantage.co/query"
ALPHA_CACHE_TTL = 300
ALPHA_SERIES_CACHE: Dict[str, tuple[List[dict], float]] = {}
SYMBOL_ALIAS_MAP: Dict[str, Tuple[str, Optional[str]]] = {
    "KOSPI": ("^KS11", "KOSPI ì§€ìˆ˜"),
    "KOSDAQ": ("^KQ11", "KOSDAQ ì§€ìˆ˜"),
    "KOSPI200": ("^KS200", "KOSPI 200"),
    "NASDAQ": ("^IXIC", "NASDAQ Composite"),
    "DOW": ("^DJI", "Dow Jones Industrial Average"),
    "S&P500": ("^GSPC", "S&P 500 Index"),
}
MARKET_REFRESH_INTERVAL = 180
NEWS_REFRESH_INTERVAL = 300
MARKET_CACHE: Dict[str, Dict[str, object]] = {}
MARKET_CACHE_LOCK = asyncio.Lock()
NEWS_CACHE: Dict[str, tuple[List[NewsArticle], float]] = {}
NEWS_CACHE_LOCK = asyncio.Lock()
MARKET_REFRESH_TASK: Optional[asyncio.Task] = None
NEWS_REFRESH_TASK: Optional[asyncio.Task] = None
NEWS_CATEGORIES = ["general"]

# RSS í”¼ë“œ URL ëª©ë¡
KOREA_NEWS_RSS = [
    "https://www.hankyung.com/feed/economy",  # í•œêµ­ê²½ì œ
    "https://www.mk.co.kr/rss/30000041/",  # ë§¤ì¼ê²½ì œ ê²½ì œ
    "https://biz.chosun.com/rss/site_biz.xml",  # ì¡°ì„ ë¹„ì¦ˆ
    "https://rss.etnews.com/Section901.xml",  # ì „ìžì‹ ë¬¸ (ìš”ì•½ í¬í•¨ ê°€ëŠ¥)
    "https://www.edaily.co.kr/rss/industry.xml",  # ì´ë°ì¼ë¦¬ ì‚°ì—…
]

USA_NEWS_RSS = [
    "https://rss.cnn.com/rss/money_latest.rss",  # CNN Money (ê°€ìž¥ ì•ˆì •ì )
    "https://feeds.bloomberg.com/markets/news.rss",  # Bloomberg Markets
    "https://www.cnbc.com/id/100003114/device/rss/rss.html",  # CNBC News
    "https://www.marketwatch.com/rss/topstories",  # MarketWatch
    "https://feeds.finance.yahoo.com/rss/2.0/headline?s=^GSPC&region=US&lang=en-US",  # Yahoo Finance S&P 500 (ë°±ì—…)
]


async def _fetch_finnhub_news(category: str) -> List[NewsArticle]:
    api_key = os.getenv("FINNHUB_API_KEY")
    if not api_key:
        raise HTTPException(status_code=500, detail="Finnhub API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    params = {"category": category, "token": api_key}

    async with httpx.AsyncClient(timeout=5.0) as client:
        response = await client.get(FINNHUB_NEWS_URL, params=params)

    if response.status_code == 429:
        raise HTTPException(status_code=429, detail="Finnhub í˜¸ì¶œ ì œí•œì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ìž ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")

    if response.status_code >= 400:
        raise HTTPException(status_code=502, detail=f"Finnhub ìš”ì²­ ì‹¤íŒ¨: {response.text}")

    data = response.json()
    articles: List[NewsArticle] = []
    for item in data[:12]:
        published_epoch = item.get("datetime")
        published_at = (
            dt.datetime.fromtimestamp(published_epoch, tz=dt.timezone.utc)
            if isinstance(published_epoch, (int, float))
            else dt.datetime.utcnow()
        )
        related_raw = item.get("related") or ""
        symbols = [symbol for symbol in related_raw.split(",") if symbol]
        headline = item.get("headline", "").strip()
        summary = (item.get("summary") or "").strip()

        # Try translation with timeout protection
        headline_ko = None
        summary_ko = None
        try:
            headline_ko = translate_to_korean(headline) or None
        except Exception:
            pass
        try:
            summary_ko = translate_to_korean(summary) or None
        except Exception:
            pass
        
        articles.append(
            NewsArticle(
                headline=headline,
                headline_ko=headline_ko,
                summary=summary or None,
                summary_ko=summary_ko,
                url=item.get("url", ""),
                source=item.get("source"),
                published_at=published_at,
                symbols=symbols,
                image=item.get("image"),
            )
        )

    if not articles:
        raise HTTPException(status_code=404, detail="Finnhubì—ì„œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    return articles


async def _fetch_rss_news(rss_urls: List[str], translate: bool = False) -> List[NewsArticle]:
    """RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    
    Args:
        rss_urls: RSS í”¼ë“œ URL ëª©ë¡
        translate: Trueì´ë©´ ë²ˆì—­ ì‹œë„ (ë¯¸êµ­ ë‰´ìŠ¤ìš©)
    """
    articles: List[NewsArticle] = []
    
    async with httpx.AsyncClient(timeout=10.0, follow_redirects=True) as client:
        for rss_url in rss_urls:
            try:
                response = await client.get(rss_url, headers={"User-Agent": "Mozilla/5.0 (compatible; RSS Reader)"})
                if response.status_code != 200:
                    logger.debug(f"RSS í”¼ë“œ ì‘ë‹µ ì‹¤íŒ¨ ({rss_url}): HTTP {response.status_code}")
                    continue
                
                feed = feedparser.parse(response.text)
                
                # í”¼ë“œê°€ ìœ íš¨í•œì§€ í™•ì¸
                if not hasattr(feed, 'entries') or not feed.entries:
                    logger.debug(f"RSS í”¼ë“œì— í•­ëª©ì´ ì—†ìŒ ({rss_url})")
                    continue
                
                for entry in feed.entries[:10]:  # ê° í”¼ë“œì—ì„œ ìµœëŒ€ 10ê°œ
                    # ë‚ ì§œ íŒŒì‹±
                    published_at = dt.datetime.utcnow()
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        try:
                            published_at = dt.datetime(*entry.published_parsed[:6], tzinfo=dt.timezone.utc)
                        except Exception:
                            pass
                    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                        try:
                            published_at = dt.datetime(*entry.updated_parsed[:6], tzinfo=dt.timezone.utc)
                        except Exception:
                            pass
                    
                    headline = entry.get("title", "").strip()
                    # summary, description, content ë“±ì—ì„œ ìš”ì•½ ì¶”ì¶œ ì‹œë„
                    summary = (
                        entry.get("summary", "").strip() 
                        or entry.get("description", "").strip()
                        or (entry.get("content", [{}])[0].get("value", "").strip() if entry.get("content") and len(entry.get("content", [])) > 0 else "")
                    )
                    url = entry.get("link", "")
                    source = entry.get("source", {}).get("title", "") if hasattr(entry, "source") else feed.feed.get("title", "")
                    
                    # ì´ë¯¸ì§€ ì¶”ì¶œ
                    image = None
                    if hasattr(entry, 'media_content') and entry.media_content:
                        image = entry.media_content[0].get('url')
                    elif hasattr(entry, 'enclosures') and entry.enclosures:
                        for enc in entry.enclosures:
                            if enc.get('type', '').startswith('image'):
                                image = enc.get('href')
                                break
                    
                    # summaryê°€ ë¹ˆ ë¬¸ìžì—´ì´ë©´ Noneìœ¼ë¡œ ì„¤ì •
                    final_summary = summary if summary else None
                    
                    # ë²ˆì—­ì´ í•„ìš”í•œ ê²½ìš° (ë¯¸êµ­ ë‰´ìŠ¤)
                    headline_ko = None
                    summary_ko = None
                    if translate:
                        # ë²ˆì—­ì€ ë‚˜ì¤‘ì— _fetch_usa_newsì—ì„œ ì²˜ë¦¬
                        headline_ko = None
                        summary_ko = None
                    else:
                        # í•œêµ­ ë‰´ìŠ¤ëŠ” ì´ë¯¸ í•œêµ­ì–´
                        headline_ko = headline
                        summary_ko = final_summary
                    
                    articles.append(
                        NewsArticle(
                            headline=headline,
                            headline_ko=headline_ko,
                            summary=final_summary,
                            summary_ko=summary_ko,
                            url=url,
                            source=source,
                            published_at=published_at,
                            symbols=[],
                            image=image,
                        )
                    )
            except Exception as e:
                logger.warning(f"RSS í”¼ë“œ íŒŒì‹± ì‹¤íŒ¨ ({rss_url}): {e}")
                continue
    
    # ë‚ ì§œìˆœìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
    articles.sort(key=lambda x: x.published_at, reverse=True)
    return articles[:20]  # ìµœëŒ€ 20ê°œ ë°˜í™˜


async def _fetch_korea_news() -> List[NewsArticle]:
    """í•œêµ­ ê²½ì œ ë‰´ìŠ¤ë¥¼ RSS í”¼ë“œì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    articles = await _fetch_rss_news(KOREA_NEWS_RSS, translate=False)
    
    # ìš”ì•½ì´ ì—†ëŠ” ê¸°ì‚¬ì— ëŒ€í•´ ì²˜ë¦¬
    result = []
    for article in articles:
        # RSS í”¼ë“œì— ìš”ì•½ì´ ì—†ìœ¼ë©´ í—¤ë“œë¼ì¸ì„ ìš”ì•½ìœ¼ë¡œ ì‚¬ìš©
        # (í•œêµ­ ë‰´ìŠ¤ RSS í”¼ë“œëŠ” ëŒ€ë¶€ë¶„ ìš”ì•½ì„ ì œê³µí•˜ì§€ ì•ŠìŒ)
        summary_value = article.summary if article.summary else article.headline
        summary_ko_value = article.summary_ko if article.summary_ko else article.headline
        
        # ìƒˆë¡œìš´ ê°ì²´ ìƒì„± (Pydantic ëª¨ë¸ì€ ë¶ˆë³€ì¼ ìˆ˜ ìžˆìŒ)
        result.append(
            NewsArticle(
                headline=article.headline,
                headline_ko=article.headline_ko,
                summary=summary_value,
                summary_ko=summary_ko_value,
                url=article.url,
                source=article.source,
                published_at=article.published_at,
                symbols=article.symbols,
                image=article.image,
            )
        )
    
    return result


async def _fetch_usa_news() -> List[NewsArticle]:
    """ë¯¸êµ­ ê²½ì œ ë‰´ìŠ¤ë¥¼ RSS í”¼ë“œì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    articles = []
    
    # RSS í”¼ë“œ ì‹œë„
    try:
        articles = await _fetch_rss_news(USA_NEWS_RSS, translate=True)
        logger.info(f"RSS í”¼ë“œì—ì„œ {len(articles)}ê°œì˜ ë¯¸êµ­ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.warning(f"RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
    
    # RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í•œ ê²½ìš° Finnhub ì‚¬ìš©
    if not articles or len(articles) == 0:
        logger.info("RSS í”¼ë“œì—ì„œ ë¯¸êµ­ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í•´ Finnhubë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        try:
            finnhub_articles = await _fetch_finnhub_news("general")
            # Finnhub ë‰´ìŠ¤ ì¤‘ ë¯¸êµ­ ê´€ë ¨ ë‰´ìŠ¤ í•„í„°ë§ (ê°„ë‹¨ížˆ ì²˜ìŒ 20ê°œ ì‚¬ìš©)
            articles = finnhub_articles[:20]
            logger.info(f"Finnhubì—ì„œ {len(articles)}ê°œì˜ ë¯¸êµ­ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.warning(f"Finnhubì—ì„œë„ ë¯¸êµ­ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {e}")
            # ìµœì†Œí•œ ë¹ˆ ë°°ì—´ì´ ì•„ë‹Œ ê¸°ë³¸ ë©”ì‹œì§€ë¼ë„ ë°˜í™˜
            if not articles:
                return []
    
    # ë¯¸êµ­ ë‰´ìŠ¤ëŠ” ì˜ì–´ì´ë¯€ë¡œ ë²ˆì—­ ì‹œë„ (ë²ˆì—­ ì‹¤íŒ¨í•´ë„ ì›ë¬¸ ë°˜í™˜)
    for article in articles:
        if article.headline and not article.headline_ko:
            try:
                translated = translate_to_korean(article.headline)
                article.headline_ko = translated if translated and translated != article.headline else article.headline
            except Exception as e:
                logger.debug(f"í—¤ë“œë¼ì¸ ë²ˆì—­ ì‹¤íŒ¨ (ì›ë¬¸ ì‚¬ìš©): {e}")
                article.headline_ko = article.headline
        
        if article.summary and not article.summary_ko:
            try:
                translated = translate_to_korean(article.summary)
                article.summary_ko = translated if translated and translated != article.summary else article.summary
            except Exception as e:
                logger.debug(f"ìš”ì•½ ë²ˆì—­ ì‹¤íŒ¨ (ì›ë¬¸ ì‚¬ìš©): {e}")
                article.summary_ko = article.summary
    
    return articles if articles else []


@app.get("/api/news", response_model=List[NewsArticle])
async def get_news(category: str = "general") -> List[NewsArticle]:
    await _ensure_news_cached(category)
    key = category.lower()

    async with NEWS_CACHE_LOCK:
        entry = NEWS_CACHE.get(key)
        if not entry and key != "general":
            entry = NEWS_CACHE.get("general")

    if not entry:
        raise HTTPException(status_code=503, detail="ë‰´ìŠ¤ ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìž ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

    return entry[0]


@app.get("/api/news/korea", response_model=List[NewsArticle])
async def get_korea_news() -> List[NewsArticle]:
    """í•œêµ­ ê²½ì œ ë‰´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    key = "korea"
    # ìºì‹œ ë¬´ì‹œí•˜ê³  í•­ìƒ ìµœì‹  ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ìš”ì•½ í¬í•¨)
    articles = await _fetch_korea_news()
    async with NEWS_CACHE_LOCK:
        NEWS_CACHE[key] = (articles, time.time())
    
    return articles


@app.get("/api/news/usa", response_model=List[NewsArticle])
async def get_usa_news() -> List[NewsArticle]:
    """ë¯¸êµ­ ê²½ì œ ë‰´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    key = "usa"
    async with NEWS_CACHE_LOCK:
        entry = NEWS_CACHE.get(key)
        if entry and time.time() - entry[1] < NEWS_REFRESH_INTERVAL:
            return entry[0]
    
    articles = await _fetch_usa_news()
    async with NEWS_CACHE_LOCK:
        NEWS_CACHE[key] = (articles, time.time())
    
    return articles


def _get_finnhub_api_key() -> str:
    api_key = os.getenv("FINNHUB_API_KEY")
    if not api_key:
        raise HTTPException(status_code=500, detail="Finnhub API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    return api_key


def _extract_error_message(response: httpx.Response) -> str:
    try:
        data = response.json()
        if isinstance(data, dict):
            return data.get("error") or data.get("detail") or response.text
    except Exception:  # noqa: BLE001
        pass
    return response.text


def _get_alpha_api_key() -> str:
    api_key = os.getenv("ALPHAVANTAGE_API_KEY")
    if not api_key:
        raise HTTPException(status_code=500, detail="Alpha Vantage API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    return api_key


async def _fetch_alpha_series(symbol: str) -> List[dict]:
    cache_entry = ALPHA_SERIES_CACHE.get(symbol.upper())
    if cache_entry:
        series, cached_at = cache_entry
        if time.time() - cached_at < ALPHA_CACHE_TTL:
            return series

    params = {
        "function": "TIME_SERIES_DAILY_ADJUSTED",
        "symbol": symbol,
        "apikey": _get_alpha_api_key(),
    }

    async with httpx.AsyncClient(timeout=10.0) as client:
        response = await client.get(ALPHAVANTAGE_URL, params=params)

    if response.status_code >= 400:
        raise HTTPException(status_code=502, detail=f"Alpha Vantage í˜¸ì¶œ ì‹¤íŒ¨: {response.text}")

    payload = response.json()
    series_raw = payload.get("Time Series (Daily)")
    if not series_raw:
        note = payload.get("Note") or payload.get("Information")
        error_message = payload.get("Error Message")
        if note:
            status = 429 if "frequency" in note.lower() else 404
            raise HTTPException(status_code=status, detail=note)
        if error_message:
            raise HTTPException(status_code=404, detail=error_message)
        raise HTTPException(status_code=404, detail=f"{symbol.upper()} ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    series: List[dict] = []
    for date_str, values in series_raw.items():
        try:
            date = dt.datetime.strptime(date_str, "%Y-%m-%d").replace(tzinfo=dt.timezone.utc)
        except ValueError:
            continue

        try:
            record = {
                "date": date,
                "open": float(values["1. open"]),
                "high": float(values["2. high"]),
                "low": float(values["3. low"]),
                "close": float(values["4. close"]),
                "adjusted_close": float(values.get("5. adjusted close", values["4. close"])),
                "volume": float(values["6. volume"]),
            }
        except (KeyError, ValueError):
            continue

        series.append(record)

    if not series:
        raise HTTPException(status_code=404, detail=f"{symbol.upper()} ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    series.sort(key=lambda item: item["date"], reverse=True)
    series = series[:500]
    ALPHA_SERIES_CACHE[symbol.upper()] = (series, time.time())
    return series


def _normalize_symbol(symbol: str, fallback_name: Optional[str] = None) -> Tuple[str, Optional[str]]:
    alias = SYMBOL_ALIAS_MAP.get(symbol.upper())
    if alias:
        provider_symbol, alias_name = alias
        return provider_symbol, alias_name or fallback_name
    return symbol, fallback_name


async def _fetch_quote(symbol: str, name: Optional[str] = None) -> MarketQuote:
    display_symbol = symbol.upper()
    provider_symbol, alias_name = _normalize_symbol(display_symbol, name)
    display_name = alias_name or name

    cache_entry = QUOTE_CACHE.get(provider_symbol.upper())
    if cache_entry:
        cached_quote, cached_at = cache_entry
        if time.time() - cached_at < CACHE_TTL_SECONDS:
            return cached_quote

    try:
        alpha_series = await _fetch_alpha_series(provider_symbol)
        quote = _quote_from_alpha_series(provider_symbol, display_symbol, display_name, alpha_series)
        QUOTE_CACHE[provider_symbol.upper()] = (quote, time.time())
        return quote
    except HTTPException as exc:
        if exc.status_code not in (404, 429):
            raise
        try:
            quote = await asyncio.to_thread(
                _fallback_quote_yfinance, provider_symbol, display_symbol, display_name
            )
            QUOTE_CACHE[provider_symbol.upper()] = (quote, time.time())
            return quote
        except HTTPException:
            raise
        except Exception as fallback_exc:  # noqa: BLE001
            if cache_entry:
                return cache_entry[0]
            raise HTTPException(status_code=exc.status_code, detail=str(fallback_exc)) from fallback_exc


def _quote_from_alpha_series(
    provider_symbol: str, display_symbol: str, display_name: Optional[str], series: List[dict]
) -> MarketQuote:
    latest = series[0]
    prev = series[1] if len(series) > 1 else latest

    current = latest["close"]
    prev_close = prev["close"]

    if prev_close:
        change = current - prev_close
        percent = (change / prev_close) * 100 if prev_close != 0 else 0.0
    else:
        change = 0.0
        percent = 0.0

    return MarketQuote(
        symbol=display_symbol,
        name=display_name or display_symbol,
        current=current,
        change=change,
        percent=percent,
        high=latest["high"],
        low=latest["low"],
        open=latest["open"],
        previous_close=prev_close,
        timestamp=latest["date"],
    )


def _candles_from_series(
    display_symbol: str, series: List[dict], range_days: int, resolution: str = "D"
) -> CandleResponse:
    lookback = max(range_days, 1)
    subset = series[:lookback]
    ordered = list(reversed(subset))

    return CandleResponse(
        symbol=display_symbol,
        resolution=resolution,
        data=CandleSeries(
            timestamps=[int(record["date"].timestamp()) for record in ordered],
            opens=[record["open"] for record in ordered],
            highs=[record["high"] for record in ordered],
            lows=[record["low"] for record in ordered],
            closes=[record["close"] for record in ordered],
            volumes=[record["volume"] for record in ordered],
        ),
    )


async def _refresh_symbol(symbol: str, name: Optional[str] = None) -> Optional[Dict[str, object]]:
    display_symbol = symbol.upper()
    provider_symbol, alias_name = _normalize_symbol(display_symbol, name)
    label = alias_name or name

    try:
        series = await _fetch_alpha_series(provider_symbol)
        quote = _quote_from_alpha_series(provider_symbol, display_symbol, label, series)
    except HTTPException as exc:
        if exc.status_code not in (404, 429):
            logger.warning("Alpha Vantage ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ (%s): %s", display_symbol, exc.detail)
            return None
        logger.info("Alpha Vantage ì œí•œìœ¼ë¡œ yfinance ì‚¬ìš© (%s)", display_symbol)
        try:
            quote = await asyncio.to_thread(
                _fallback_quote_yfinance, provider_symbol, display_symbol, label
            )
            series, candles = await asyncio.to_thread(
                _fallback_candles_yfinance, provider_symbol, display_symbol, "D", 60
            )
        except HTTPException as fallback_exc:
            logger.warning("yfinance ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ (%s): %s", display_symbol, fallback_exc.detail)
            return None
    else:
        candles = _candles_from_series(display_symbol, series, 60)
    CANDLE_CACHE[(provider_symbol.upper(), "D", 60)] = (candles, time.time())
    return {
        "quote": quote,
        "series": series,
        "updated_at": dt.datetime.utcnow(),
    }


async def _ensure_symbol_cached(symbol: str, name: Optional[str] = None) -> None:
    display_symbol = symbol.upper()
    async with MARKET_CACHE_LOCK:
        entry = MARKET_CACHE.get(display_symbol)
        if entry:
            updated_at: dt.datetime = entry["updated_at"]  # type: ignore[assignment]
            if (dt.datetime.utcnow() - updated_at).total_seconds() < CACHE_TTL_SECONDS:
                return

    refreshed = await _refresh_symbol(symbol, name)
    if refreshed:
        async with MARKET_CACHE_LOCK:
            MARKET_CACHE[display_symbol] = refreshed


async def _refresh_market_cache_once() -> None:
    for symbol, name in MARKET_OVERVIEW_SYMBOLS:
        refreshed = await _refresh_symbol(symbol, name)
        if refreshed:
            async with MARKET_CACHE_LOCK:
                MARKET_CACHE[symbol.upper()] = refreshed
        await asyncio.sleep(15)


async def _refresh_news_category(category: str) -> Optional[List[NewsArticle]]:
    try:
        articles = await _fetch_finnhub_news(category)
    except HTTPException as exc:
        logger.warning("ë‰´ìŠ¤ ê°±ì‹  ì‹¤íŒ¨(%s): %s", category, exc.detail)
        return None

    async with NEWS_CACHE_LOCK:
        NEWS_CACHE[category.lower()] = (articles, time.time())
    return articles


async def _refresh_news_cache_once() -> None:
    for category in NEWS_CATEGORIES:
        await _refresh_news_category(category)


async def _market_refresh_loop() -> None:
    while True:
        try:
            await _refresh_market_cache_once()
        except Exception as exc:  # noqa: BLE001
            logger.exception("ì‹œìž¥ ë°ì´í„° ê°±ì‹  ë£¨í”„ ì˜¤ë¥˜: %s", exc)
        await asyncio.sleep(MARKET_REFRESH_INTERVAL)


async def _news_refresh_loop() -> None:
    while True:
        try:
            await _refresh_news_cache_once()
        except Exception as exc:  # noqa: BLE001
            logger.exception("ë‰´ìŠ¤ ë°ì´í„° ê°±ì‹  ë£¨í”„ ì˜¤ë¥˜: %s", exc)
        await asyncio.sleep(NEWS_REFRESH_INTERVAL)


async def _ensure_news_cached(category: str) -> None:
    key = category.lower()
    async with NEWS_CACHE_LOCK:
        entry = NEWS_CACHE.get(key)
        if entry and time.time() - entry[1] < NEWS_REFRESH_INTERVAL:
            return
    await _refresh_news_category(category)


@app.get("/api/market/overview", response_model=List[MarketQuote])
async def market_overview() -> List[MarketQuote]:
    async with MARKET_CACHE_LOCK:
        results = []
        missing: List[Tuple[str, str]] = []
        for symbol, name in MARKET_OVERVIEW_SYMBOLS:
            entry = MARKET_CACHE.get(symbol.upper())
            if entry:
                results.append(entry["quote"])  # type: ignore[index]
            else:
                missing.append((symbol, name))

    for symbol, name in missing:
        asyncio.create_task(_refresh_symbol(symbol, name))

    if not results:
        raise HTTPException(status_code=503, detail="ì‹œìž¥ ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìž ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
    return results


@app.get("/api/market/quote", response_model=MarketQuote)
async def market_quote(symbol: str = Query(..., description="ì¡°íšŒí•  ì¢…ëª© í‹°ì»¤")) -> MarketQuote:
    await _ensure_symbol_cached(symbol)

    async with MARKET_CACHE_LOCK:
        entry = MARKET_CACHE.get(symbol.upper())

    if not entry:
        raise HTTPException(status_code=404, detail=f"{symbol.upper()} ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    return entry["quote"]  # type: ignore[index]


@app.get("/api/market/search", response_model=List[SymbolSearchResult])
async def market_search(query: str = Query(..., min_length=1, description="ì‹¬ë³¼ ë˜ëŠ” ì¢…ëª©ëª… ê²€ìƒ‰ì–´")) -> List[SymbolSearchResult]:
    api_key = _get_finnhub_api_key()
    async with httpx.AsyncClient(timeout=5.0) as client:
        response = await client.get(FINNHUB_SEARCH_URL, params={"q": query, "token": api_key})

    if response.status_code == 429:
        raise HTTPException(status_code=429, detail="Finnhub í˜¸ì¶œ ì œí•œì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ìž ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
    if response.status_code >= 400:
        raise HTTPException(status_code=502, detail=f"Finnhub ê²€ìƒ‰ ì‹¤íŒ¨: {response.text}")

    payload = response.json()
    results = payload.get("result") or []

    formatted = [
        SymbolSearchResult(
            symbol=item.get("symbol", "").upper(),
            description=item.get("description", "").strip(),
            type=item.get("type"),
            exchange=item.get("exchange"),
        )
        for item in results
        if item.get("symbol")
    ]

    return formatted[:15]


@app.get("/api/market/candles", response_model=CandleResponse)
async def market_candles(
    symbol: str = Query(..., description="ì¡°íšŒí•  ì¢…ëª© í‹°ì»¤"),
    resolution: str = Query("15", description="Finnhub ìº”ë“¤ í•´ìƒë„ (1,5,15,30,60,240,D,W,M)"),
    range_days: int = Query(5, ge=1, le=60, description="ì¡°íšŒ ê¸°ê°„(ì¼)"),
) -> CandleResponse:
    normalized_resolution = resolution.upper()
    if normalized_resolution not in {"D", "1D"}:
        raise HTTPException(status_code=400, detail="ì¼ë´‰(D) í•´ìƒë„ë§Œ ì§€ì›í•©ë‹ˆë‹¤.")

    await _ensure_symbol_cached(symbol)

    async with MARKET_CACHE_LOCK:
        entry = MARKET_CACHE.get(symbol.upper())

    if not entry:
        raise HTTPException(status_code=404, detail=f"{symbol.upper()} ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    series: List[dict] = entry["series"]  # type: ignore[assignment]
    return _candles_from_series(symbol.upper(), series, range_days, normalized_resolution)


@app.get("/healthz")
def health_check() -> dict[str, str]:
    return {"status": "ok"}


@app.on_event("startup")
async def _on_startup() -> None:
    global MARKET_REFRESH_TASK, NEWS_REFRESH_TASK
    MARKET_REFRESH_TASK = asyncio.create_task(_market_refresh_loop())
    NEWS_REFRESH_TASK = asyncio.create_task(_news_refresh_loop())
    asyncio.create_task(_refresh_market_cache_once())
    asyncio.create_task(_refresh_news_cache_once())


@app.on_event("shutdown")
async def _on_shutdown() -> None:
    tasks = [MARKET_REFRESH_TASK, NEWS_REFRESH_TASK]
    for task in tasks:
        if task:
            task.cancel()
    for task in tasks:
        if task:
            with contextlib.suppress(asyncio.CancelledError):
                await task


def _fallback_quote_yfinance(
    provider_symbol: str, display_symbol: str, display_name: Optional[str]
) -> MarketQuote:
    cache_entry = QUOTE_CACHE.get(provider_symbol.upper())
    if cache_entry and time.time() - cache_entry[1] < CACHE_TTL_SECONDS:
        return cache_entry[0]

    try:
        df = _yf_download_with_retry(provider_symbol, "5d", "1d")
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=404, detail=f"{display_symbol} ì‹œì„¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.") from exc

    if df.empty:
        raise HTTPException(status_code=404, detail=f"{display_symbol} ì‹œì„¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    df = df.dropna()
    if df.empty:
        raise HTTPException(status_code=404, detail=f"{display_symbol} ì‹œì„¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    last = df.iloc[-1]
    prev = df.iloc[-2] if len(df) > 1 else last

    prev_close = float(prev["Close"]) if not pd.isna(prev["Close"]) else None
    current = float(last["Close"])

    if prev_close and prev_close != 0:
        change = current - prev_close
        percent = (change / prev_close) * 100
    else:
        change = 0.0
        percent = 0.0

    timestamp = df.index[-1]
    if isinstance(timestamp, pd.Timestamp):
        timestamp = timestamp.to_pydatetime()
    if timestamp.tzinfo is None:
        timestamp = timestamp.replace(tzinfo=dt.timezone.utc)

    quote = MarketQuote(
        symbol=display_symbol,
        name=display_name or display_symbol,
        current=current,
        change=change,
        percent=percent,
        high=float(last["High"]) if not pd.isna(last["High"]) else None,
        low=float(last["Low"]) if not pd.isna(last["Low"]) else None,
        open=float(last["Open"]) if not pd.isna(last["Open"]) else None,
        previous_close=prev_close,
        timestamp=timestamp,
    )
    QUOTE_CACHE[provider_symbol.upper()] = (quote, time.time())
    return quote


def _map_interval_and_period(resolution: str, range_days: int) -> tuple[str, str]:
    res = resolution.upper()
    if res == "1":
        return "1m", "5d"
    if res == "5":
        return "5m", f"{min(range_days, 30)}d"
    if res == "15":
        return "15m", f"{min(range_days, 30)}d"
    if res == "30":
        return "30m", f"{min(range_days, 60)}d"
    if res == "60":
        return "60m", f"{min(range_days, 60)}d"
    if res == "240":
        return "1h", _period_from_days(range_days)
    if res in {"D", "1D"}:
        return "1d", _period_from_days(range_days)
    if res in {"W", "1W"}:
        return "1wk", _period_from_days(range_days)
    if res in {"M", "1M"}:
        return "1mo", _period_from_days(range_days)
    return "1d", _period_from_days(range_days)


def _period_from_days(days: int) -> str:
    if days <= 5:
        return "5d"
    if days <= 30:
        return "1mo"
    if days <= 90:
        return "3mo"
    if days <= 180:
        return "6mo"
    if days <= 365:
        return "1y"
    if days <= 730:
        return "2y"
    if days <= 1825:
        return "5y"
    return "10y"


def _fallback_candles_yfinance(
    provider_symbol: str, display_symbol: str, resolution: str, range_days: int
) -> tuple[List[dict], CandleResponse]:
    cache_key = (provider_symbol.upper(), resolution, range_days)
    cache_entry = CANDLE_CACHE.get(cache_key)
    if cache_entry and time.time() - cache_entry[1] < CACHE_TTL_SECONDS:
        candles = cache_entry[0]
        records = [
            {
                "date": dt.datetime.fromtimestamp(ts, tz=dt.timezone.utc),
                "open": opens,
                "high": highs,
                "low": lows,
                "close": closes,
                "volume": volumes,
            }
            for ts, opens, highs, lows, closes, volumes in zip(
                candles.data.timestamps,
                candles.data.opens,
                candles.data.highs,
                candles.data.lows,
                candles.data.closes,
                candles.data.volumes,
            )
        ]
        records.sort(key=lambda item: item["date"], reverse=True)
        return records, candles

    primary_interval, primary_period = _map_interval_and_period(resolution, range_days)
    fallback_candidates: List[tuple[str, str]] = [
        (primary_interval, primary_period),
    ]

    if primary_interval not in {"1d", "1wk", "1mo"}:
        fallback_candidates.append(("1d", _period_from_days(range_days)))
    fallback_candidates.append(("1wk", _period_from_days(max(range_days, 30))))

    last_error: Optional[Exception] = None

    for interval, period in fallback_candidates:
        try:
            df = _yf_download_with_retry(provider_symbol, period, interval)
        except Exception as exc:  # noqa: BLE001
            last_error = exc
            continue

        if df.empty:
            last_error = ValueError("empty dataframe")
            continue

        df = df.dropna()
        if df.empty:
            last_error = ValueError("empty dataframe after dropna")
            continue

        timestamps: List[int] = []
        for idx in df.index:
            ts = idx.to_pydatetime() if isinstance(idx, pd.Timestamp) else pd.Timestamp(idx).to_pydatetime()
            if ts.tzinfo is None:
                ts = ts.replace(tzinfo=dt.timezone.utc)
            timestamps.append(int(ts.timestamp()))

        records = []
        for idx, ts_value in zip(df.index, timestamps):
            ts_dt = dt.datetime.fromtimestamp(ts_value, tz=dt.timezone.utc)
            row = df.loc[idx]
            records.append(
                {
                    "date": ts_dt,
                    "open": float(row["Open"]),
                    "high": float(row["High"]),
                    "low": float(row["Low"]),
                    "close": float(row["Close"]),
                    "volume": float(row["Volume"]),
                }
            )

        records.sort(key=lambda item: item["date"], reverse=True)
        candles = _candles_from_series(display_symbol, records, range_days, resolution)
        CANDLE_CACHE[cache_key] = (candles, time.time())
        return records, candles

    detail = f"{display_symbol} ì°¨íŠ¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    if last_error:
        detail = f"{detail} (fallback ì‹¤íŒ¨: {last_error})"
    raise HTTPException(status_code=404, detail=detail)


def _yf_download_with_retry(symbol: str, period: str, interval: str, attempts: int = 3) -> pd.DataFrame:
    delay = 1.0
    last_exc: Optional[Exception] = None

    for _ in range(attempts):
        try:
            df = yf.download(
                symbol,
                period=period,
                interval=interval,
                progress=False,
                auto_adjust=False,
                threads=False,
            )
            return df
        except Exception as exc:  # noqa: BLE001
            last_exc = exc
            time.sleep(delay)
            delay *= 2

    if last_exc:
        raise last_exc
    return pd.DataFrame()

